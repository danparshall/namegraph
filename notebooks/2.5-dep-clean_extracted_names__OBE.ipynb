{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import datetime as dt\n",
    "\n",
    "import unidecode\n",
    "from fuzzywuzzy import fuzz    \n",
    "    \n",
    "# enable progress bar on long operations\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_run = False\n",
    "N_ROWS = None # 100000\n",
    "READ_DATE = '20200824'\n",
    "READ_DATE = '20200917'\n",
    "\n",
    "\n",
    "LOC_RAW = \"../data/raw/\"\n",
    "LOC_INTERIM = \"../data/interim/\"\n",
    "TODAY = dt.datetime.now().strftime(\"%Y%m%d\")\n",
    "TODAY"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "nan_values = ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', # 'NA' is sometimes name\n",
    "              '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', '']\n",
    "\n",
    "dtypes_reg = {'cedula':str,\n",
    "                'nombre':str,\n",
    "                'gender':'category',\n",
    "                'dt_birth':str,\n",
    "                'nationality':'category',\n",
    "                'nombre_spouse':str,\n",
    "                'dt_death':str,\n",
    "                'orig_cedula':str,\n",
    "                'marital_status':str,\n",
    "                'nombre_padre':str,\n",
    "                'nombre_madre':str,\n",
    "                'ced_spouse':str,\n",
    "                'ced_padre':str,\n",
    "                'ced_madre':str,\n",
    "                'dt_marriage':str,\n",
    "             }\n",
    "\n",
    "\n",
    "if True:\n",
    "    # read cleaned-up input file\n",
    "    print(\"LOADING REG DATA FOR : \" + READ_DATE)\n",
    "    dtypes_reg = {'cedula':str, 'nombre':str, 'gender':'category', 'nationality':'category', \n",
    "                 'orig_cedula':str, 'marital_status':'category', \n",
    "                  'nombre_spouse':str, 'nombre_padre':str, 'nombre_madre':str,\n",
    "                  'ced_spouse':str, 'ced_padre':str, 'ced_madre':str\n",
    "                 }\n",
    "    \n",
    "    rf = pd.read_csv(LOC_RAW + \"REG_NAMES_current.tsv\", sep='\\t', dtype=dtypes_reg,\n",
    "                     parse_dates=['dt_birth','dt_death','dt_marriage'],\n",
    "                     keep_default_na=False, na_values=nan_values,\n",
    "                    )\n",
    "    if \"index\" in rf.columns:\n",
    "        rf.drop(columns=['index'], inplace=True)\n",
    "\n",
    "        \n",
    "# replace NaN with empty string\n",
    "text_cols = ['nombre', 'nombre_spouse', 'nombre_padre', 'nombre_madre', 'ced_spouse', 'ced_padre', 'ced_madre']\n",
    "rf[text_cols] = rf[text_cols].fillna(\"\")\n",
    "\n",
    "print(\"# records loaded :\", len(rf))\n",
    "# 5 mins"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funcs to parse primary citizen row-by-row"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def is_double_name(guess):\n",
    "    if len(tokens) == 2*len(set(tokens)):\n",
    "    # case when sur1 == sur2, so just take the first one as father's name\n",
    "        appelido = ' '.join(tokens[:int(len(tokens)/2)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_madre_doubling(madre):\n",
    "    mparts = madre.split()\n",
    "    n_mparts = len(mparts)\n",
    "    \n",
    "    if n_mparts == len(set(mparts)):\n",
    "        # no possible doubling, use name as-is\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # check for doublings, looking at both the front and the end of the string\n",
    "        sur_madre = None\n",
    "        for i in range(1, int(np.floor(n_mparts-1))):\n",
    "            if mparts[:i] == mparts[i: 2*i]:\n",
    "                # legal form\n",
    "                sur_madre = ' '.join(mparts[:i])\n",
    "                break\n",
    "            elif mparts[-i:] == mparts[-2*i:-i] :\n",
    "                # sequential form\n",
    "                sur_madre = ' '.join(mparts[-i:])\n",
    "                break\n",
    "    return sur_madre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_nombre_doubling(nombre):\n",
    "    tokens = nombre.split()\n",
    "    num_parts = len(tokens)\n",
    "    if num_parts == len(set(tokens)):\n",
    "        # no possible doubling, use name as-is\n",
    "        return \"\"\n",
    "    \n",
    "    else:\n",
    "        # check for doublings, looking at both the front and the end of the string\n",
    "        surname = \"\"\n",
    "        for i in range(1, num_parts-1):\n",
    "            if tokens[:i] == tokens[i: 2*i]:\n",
    "                # legal form\n",
    "                surname = ' '.join(tokens[:i])\n",
    "                break\n",
    "            elif tokens[-i:] == tokens[-2*i:-i] :\n",
    "                # sequential form\n",
    "                surname = ' '.join(tokens[-i:])\n",
    "                break\n",
    "    return surname"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_substrings(nombre, START=0):\n",
    "    nlen = len(nombre)\n",
    "    n_max = nlen - START\n",
    "    for chunk_len in range(n_max, 0, -1):\n",
    "        n_subs = n_max - chunk_len + 1\n",
    "        for ind in range(0,n_subs):\n",
    "            sub = ' '.join(nombre[START+ind : ind+chunk_len])\n",
    "    \n",
    "            # only return a single-token substring if the length is at least 2 (avoids matching on \"DE\", etc)\n",
    "            if (chunk_len > 1) or (len(sub) > 2):\n",
    "                yield sub"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re_de_pre   = re.compile(r\"(^.*)\\s(DEL?\\s\\w+.*$)\")\n",
    "re_one_y_two = re.compile(r\"(^|\\s)\\w{2,} [IYE]{1} \\w{2,}(\\s|$)\")\n",
    "re_de_la_ao = re.compile(r\"(^|\\s)DE LA [AO]+(\\s|$)\")\n",
    "\n",
    "def parse_fullrow(row):\n",
    "    \n",
    "    out = {'cedula':'', 'sur_padre':\"\", 'sur_madre':\"\", \"prenames\":\"\", \n",
    "            \"has_padre\":False, \"is_plegal\":None,\n",
    "            \"has_madre\":False, \"is_mlegal\":None}\n",
    "    out['cedula'] = row.cedula\n",
    "\n",
    "    if not row.nombre_padre and not row.nombre_madre:\n",
    "        return out\n",
    "\n",
    "    nombre = row.nombre\n",
    "    nomset = set(nombre.split())\n",
    "    madre = \"\"\n",
    "    padre = \"\"\n",
    "    prenames = \"\"\n",
    "    \n",
    "    # check if madre/padre have overlapping tokens (requires special handling)\n",
    "    mset = set(row.nombre_madre.split())\n",
    "    pset = set(row.nombre_padre.split())\n",
    "    both = pset & mset\n",
    "    flag_overlap = len(both) > 0\n",
    "\n",
    "    if flag_overlap:\n",
    "        surname = check_nombre_doubling(nombre)\n",
    "        if surname != \"\":\n",
    "            madre = surname\n",
    "            padre = surname\n",
    "        else:\n",
    "            for guess in get_substrings(row.nombre_madre.split()):\n",
    "                if (guess in nombre\n",
    "                and not nombre.endswith(guess) ):\n",
    "                    poss_padre = nombre.split(guess)[0].strip()\n",
    "                    poss_set = set(poss_padre.split())\n",
    "                    guess_set = set(guess.split())\n",
    "                    if (poss_padre in row.nombre_padre\n",
    "                        and not nombre.endswith(poss_padre) \n",
    "                        and poss_set.issubset(nomset) and poss_set.issubset(pset)\n",
    "                        and guess_set.issubset(nomset) and guess_set.issubset(mset)\n",
    "                        and poss_padre not in {'DE', 'LA', 'LOS', 'LAS', 'VAN', 'VON', 'DE LA', 'DEL'}\n",
    "                       ):\n",
    "                        padre = poss_padre\n",
    "                        madre = guess\n",
    "                        break\n",
    "\n",
    "\n",
    "    # if the overlap method wasn't successful, parse them separately\n",
    "    if not madre and not padre:\n",
    "        parts = nombre.split()\n",
    "        \n",
    "        #### FATHERS NAME ####\n",
    "        # start by trying the first LEN-1 tokens as a single name, then LEN-2 tokens, etc\n",
    "        # this matches longest chunk found, so it should pick up compound names like DE LA CRUZ\n",
    "        try:\n",
    "            if row.nombre_padre:\n",
    "                poss_padre = check_nombre_doubling(row.nombre_padre)\n",
    "                poss_pset = set(poss_padre.split())\n",
    "                if (poss_padre \n",
    "                and (poss_padre in row.nombre)\n",
    "                and not nombre.endswith(poss_padre) \n",
    "                and poss_pset.issubset(nomset)):\n",
    "                    padre = poss_padre\n",
    "                    parts = ''.join(nombre.split(padre, maxsplit=1)).strip().split()\n",
    "                else:\n",
    "                    # start by trying everything except the last element (always a prename), and work down\n",
    "                    for ind in range(len(parts)-1, 0, -1):\n",
    "                        guess = ' '.join(parts[:ind])\n",
    "                        poss_pset = set(guess.split())\n",
    "                        if ( (guess in row.nombre_padre) \n",
    "                            and poss_pset.issubset(nomset) and poss_pset.issubset(pset)\n",
    "                           ):\n",
    "                            # update before checking mother's name\n",
    "                            padre = guess\n",
    "                            parts = nombre.split(padre, maxsplit=1)[1].split()\n",
    "                            break\n",
    "        except:\n",
    "            out['sur_padre'] = \"WTF PADRE PROBLEM\"\n",
    "            return out\n",
    "\n",
    "        #### MOTHERS NAME ####\n",
    "        # having removed the padre name from the front of the string, try similar trick with the madre name\n",
    "        try:\n",
    "            if row.nombre_madre:\n",
    "                poss_madre = check_nombre_doubling(row.nombre_madre)\n",
    "                poss_mset = set(poss_madre.split())\n",
    "                if (poss_madre \n",
    "                and (poss_madre in row.nombre) \n",
    "                and not nombre.endswith(poss_madre) \n",
    "                and poss_mset.issubset(nomset) and poss_mset.issubset(mset)):\n",
    "                    madre = poss_madre\n",
    "                elif not madre:\n",
    "                    nombre_madre = row.nombre_madre\n",
    "                    \n",
    "                    if nombre_madre.startswith(parts[0]):\n",
    "                        # in legal form, so strike any catholic addons from both citizen and mother\n",
    "                        # complicated bc surnames like \"GOMEZ DE LA TORRE\" mean we have to skip the zeroth token\n",
    "                        m_de_pre_nombre = re_de_pre.match(' '.join(parts[1:]))\n",
    "                        if m_de_pre_nombre:\n",
    "                            # keep 'parts' as a list\n",
    "                            parts = parts[:1] + m_de_pre_nombre.group(1).split()\n",
    "\n",
    "                        mom_parts = nombre_madre.split()\n",
    "                        m_de_pre_madre = re_de_pre.match(' '.join(mom_parts[1:]))\n",
    "                        if m_de_pre_madre:\n",
    "                            # keep 'nombre_madre' as string\n",
    "                            nombre_madre = mom_parts[0] + \" \" + m_de_pre_madre.group(1)\n",
    "\n",
    "                    for ind in range(len(parts)-1, 0, -1):\n",
    "                        guess = ' '.join(parts[:ind])\n",
    "                        poss_mset = set(guess.split())\n",
    "                        if ((guess in nombre_madre) \n",
    "                            and not nombre.endswith(guess) \n",
    "                            and poss_mset.issubset(nomset) and poss_mset.issubset(mset)):\n",
    "                            # now check which is the better fit\n",
    "                            if (guess == nombre_madre):\n",
    "                                # when madre is in short legal form and daughter has the same prename1\n",
    "                                # it can look like the mother's surname is \"GONZALEZ MARIA\", etc.  \n",
    "                                # So ignore these\n",
    "                                pass\n",
    "                            else:\n",
    "                                madre = guess\n",
    "                                break\n",
    "                    if False:\n",
    "                        # sometimes we haven't found anything bc father's name isn't listed, \n",
    "                        # even though it's part of the citizen name\n",
    "                        if not padre and not madre and not row.nombre_padre:\n",
    "                            print(\"nada\")\n",
    "                            for guess in get_substrings(row.nombre_madre.split()):\n",
    "    #                            print('madsubguess :', guess)\n",
    "                                poss_mset = set(guess.split())\n",
    "                                if ((guess in nombre) \n",
    "                                and not nombre.endswith(guess) \n",
    "                                and poss_mset.issubset(nomset) and poss_mset.issubset(mset) ):\n",
    "                                    madre = guess\n",
    "                                    padre = nombre.split(madre)[0].strip()\n",
    "                                    print(\"M :\", madre, \"; P :\", padre)\n",
    "                                    break\n",
    "    \n",
    "        except:\n",
    "            out['sur_madre'] = \"WTF MADRE PROBLEM\"\n",
    "            return out\n",
    "        \n",
    "    # get prenames explicitly, as remainder after removing prenames.\n",
    "    # this bypasses some funny stuff I have to do above\n",
    "    deduced_surnames = (padre + \" \" + madre).strip()\n",
    "    if deduced_surnames and row.nombre.startswith(deduced_surnames):\n",
    "        prenames = ''.join([x.strip() for x in row.nombre.split(deduced_surnames)])\n",
    "    else:\n",
    "        prenames = \"WTF SURNAME PROBLEM\"\n",
    "\n",
    "        \n",
    "    if padre:\n",
    "        out['has_padre'] = True\n",
    "        out['sur_padre'] = padre\n",
    "        out['is_plegal'] = row.nombre_padre.startswith(padre)\n",
    "    if madre:\n",
    "        out['has_madre'] = True\n",
    "        out['sur_madre'] = madre\n",
    "        out['is_mlegal'] = row.nombre_madre.startswith(madre)\n",
    "    out['prenames'] = prenames\n",
    "\n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract surnames/prenames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "if full_run:\n",
    "    surnames_extracted = rf.progress_apply(lambda row: parse_fullrow(row), axis=1, result_type='expand')\n",
    "    surnames_extracted.to_csv(LOC_INTERIM + \"surnames_extracted_\" + TODAY + \".tsv\", sep='\\t', index=False)\n",
    "    # ~2h for full set\n",
    "else:\n",
    "    dtypes_names = {'cedula':str, 'sur_padre':str, 'sur_madre':str, 'prenames':str,\n",
    "                   'has_padre':bool, 'has_madre':bool, 'is_plegal':'category', 'is_mlegal':'category'\n",
    "                   }\n",
    "    surnames_extracted = pd.read_csv(LOC_INTERIM + \"surnames_extracted_\" + READ_DATE + \".tsv\", \n",
    "                                     sep='\\t', dtype=dtypes_names)\n",
    "    textcols = ['sur_padre','sur_madre','prenames']\n",
    "    surnames_extracted[textcols] = surnames_extracted[textcols].fillna(\"\")\n",
    "print(\"# parsed :\", len(surnames_extracted))\n",
    "\n",
    "# set column order\n",
    "surnames_extracted = surnames_extracted[['cedula','sur_padre','has_padre','is_plegal',\n",
    "                                         'sur_madre','has_madre','is_mlegal','prenames']]\n",
    "\n",
    "# 3.5 hours"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clean up extracted\n",
    "\n",
    "## Clean surnames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "## the \"nf\" (name frame) is just the subset of well-behaved names\n",
    "\n",
    "# confirm that the indexing is still correct\n",
    "if not (rf.index == surnames_parsed.index).all():\n",
    "    rf.reset_index(inplace=True, drop=True)\n",
    "assert (rf.cedula == surnames_parsed.cedula).all()\n",
    "\n",
    "# join the parsed names to the originals (but only retain the well-behaved ones)\n",
    "nf = pd.concat([rf[['cedula','nombre','nombre_padre','nombre_madre','gender']], surnames_parsed.iloc[:,1:]], axis=1)\n",
    "\n",
    "nf = nf.loc[(nf.sur_padre.notnull()) & (nf.sur_padre != \"\") & nf.has_padre & \n",
    "            (nf.sur_madre.notnull()) & (nf.sur_madre != \"\") & nf.has_madre & \n",
    "            (nf.prenames.notnull() & (nf.prenames != \"\")),\n",
    "        ['cedula','nombre','prenames', 'gender', \n",
    "         'nombre_padre','sur_padre','has_padre', 'is_plegal',\n",
    "         'nombre_madre','sur_madre','has_madre', 'is_mlegal']]\n",
    "\n",
    "nf['is_plegal'] = nf.is_plegal.astype(bool)\n",
    "nf['is_mlegal'] = nf.is_mlegal.astype(bool)\n",
    "len(nf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf[nf.is_plegal & ~nf.is_mlegal]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(nf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Look for very uncommon names, make sure they're legit, as opposed to parsing errors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sur_counts = nf.sur_padre.value_counts()\n",
    "sur_counts.sort_index(inplace=True)\n",
    "\n",
    "sur_counts = pd.concat([sur_counts, \n",
    "                        pd.Series(data=sur_counts.index.map(lambda x: len(x.split())), index=sur_counts.index)\n",
    "                       ], axis=1)\n",
    "sur_counts.columns = ['n_obs', 'nlen']\n",
    "\n",
    "sur_counts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 7k cases of names w <10 obs, 5100 with only 1\n",
    "# 3900 cases w < 10 obs, only 2600 with just 1 (following improvements)\n",
    "\n",
    "sur_counts[(sur_counts.nlen > 1) & (sur_counts.n_obs < 10)].sample(60)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean prenames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# in all cases, we look for a word boundary as the first group, then our funky name as the second\n",
    "re_von = re.compile(u\"(^|\\s)(V[AO]N \\w{2,})(\\s|$)\")              # these results are subset of \"re_vande\"\n",
    "re_vande = re.compile(u\"(^|\\s)(V[AO]N DE[RN]? \\w{2,})(\\s|$)\")\n",
    "re_sant = re.compile(u\"(^|\\s)(SANT?A? \\w{2,})(\\s|$)\")            # SAN and SANTA (SANTO doesn't form compounds)\n",
    "re_dela = re.compile(u\"(^|\\s)(DE L[AO]S? ?[AO]? ?\\w{2,})(\\s|$)\")   # these results are subset of \"re_laos\"\n",
    "re_laos = re.compile(u\"(^|\\s)(L[AEO]S? \\w{2,})(\\s|$)\")\n",
    "re_del  = re.compile(u\"(^|\\s)(DEL \\w{2,})(\\s|$)\")\n",
    "re_de   = re.compile(r\"(^|\\s)(DE \\w{2,})(\\s|$)\")\n",
    "\n",
    "\n",
    "def regex_funky_prenames(nombre):\n",
    "    \"\"\" This is a little slow (~4mins / million rows), but pretty thorough.  \"\"\"\n",
    "    \n",
    "    mdel   = re_del.search(nombre)\n",
    "    msant  = re_sant.search(nombre)\n",
    "    \n",
    "    mlaos  = re_laos.search(nombre)\n",
    "    mdela  = re_dela.search(nombre)\n",
    "    \n",
    "    mvon   = re_von.search(nombre)\n",
    "    mvande = re_vande.search(nombre)\n",
    "    \n",
    "    mde    = re_de.search(nombre)\n",
    "    \n",
    "    poss_funks = set()\n",
    "    \n",
    "    if mdel:\n",
    "        poss_funks.add(mdel.group(2))\n",
    "    if msant:\n",
    "        poss_funks.add(msant.group(2))\n",
    "    if mvon:\n",
    "        # \"VAN DE\" types are a subset of \"VAN\" types\n",
    "        if mvande:\n",
    "            poss_funks.add(mvande.group(2))\n",
    "        else:\n",
    "            poss_funks.add(mvon.group(2))\n",
    "    if mlaos:\n",
    "        # \"DE LA\" type names are a subset of \"LA\" types\n",
    "        if mdela:\n",
    "            poss_funks.add(mdela.group(2))\n",
    "        else:\n",
    "            poss_funks.add(mlaos.group(2))\n",
    "    if mde:\n",
    "        poss_funks.add(mde.group(2))\n",
    "\n",
    "    if poss_funks:\n",
    "        for funk in poss_funks:\n",
    "            funky_prenames.add(funk)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "def fix_funk(nombre, funks):\n",
    "    \"\"\" The 'funks' list should be sorted in descending length, to prevent substrings from being clobbered.\n",
    "    \n",
    "    NB: there's a potential bug in here, bc the list is sorted according to character length, but checks\n",
    "    here are being done according to number of tokens.  But very unlikely to cause an issue, so ignoring for now\n",
    "    \"\"\"\n",
    "    nlen = len(nombre.split())\n",
    "    for funk in funks:\n",
    "        flen = len(funk.split())\n",
    "        if (nlen > flen):\n",
    "            if (funk in nombre):\n",
    "                defunk = '_'.join(funk.split())\n",
    "                nombre = defunk.join(nombre.split(funk))\n",
    "                nlen = len(nombre.split())\n",
    "        else:\n",
    "            # since the list is sorted, once we have a match that uses all the tokens, just skip ahead\n",
    "            continue\n",
    "    return nombre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "if True: #full_run:\n",
    "    funky_prenames = set()\n",
    "    nf['is_funky'] = nf.prenames.map(regex_funky_prenames)\n",
    "    funky_prenames = list(funky_prenames)\n",
    "    \n",
    "    # ones I spotted that didn't fit one of the regex patterns\n",
    "    funky_prenames.extend([\"DEL NINO JESUS\", \"DE EL CISNE\", \"DE EL ROCIO\", \"DE LA FE\"\n",
    "                          ])\n",
    "\n",
    "    with open(LOC_INTERIM + \"funky_prenames_\" + TODAY + \".txt\", 'w') as f:\n",
    "        for funk in funky_prenames:\n",
    "            f.write(funk + \"\\n\")\n",
    "else:\n",
    "    funky_prenames = set()\n",
    "    with open(LOC_INTERIM + \"funky_prenames_\" + READ_DATE + \".txt\") as f:\n",
    "        for line in f:\n",
    "            funky_prenames.add(line.strip())\n",
    "\n",
    "\n",
    "funky_prenames.sort(reverse=True, key=len)\n",
    "print(\"# funkies :\", len(funky_prenames))\n",
    "# "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#funky_prenames[:5] + \n",
    "funky_prenames[-20:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re_chk  = re.compile(r\"(^|\\s)(LO SIU)(\\s|$)\")\n",
    "nf[nf.prenames.map(lambda x: True if re_chk.search(x) else False) & nf.is_funky]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re_de.search(\"MARIA DE LL GRAZZIA\").group(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(nf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "nf.loc[nf.is_funky, 'prenames'] = nf[nf.is_funky].prenames.progress_map(lambda x: fix_funk(x, funky_prenames))\n",
    "\n",
    "# 6 minutes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf.is_funky.sum() / len(nf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf[nf.is_funky]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re_under_space = re.compile(r\".*_\\w+\\s\\w+$\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "uspace = nf[nf.prenames.map(lambda x: True if re_under_space.match(x) else False)]  # ANULADA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf[rf.nombre.map(lambda x : \"MARIA JESUS\" in x)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(nf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "nf['nlen_padre'] = nf.nombre_padre.map(lambda x: len(x.split()))\n",
    "nf['nlen_madre'] = nf.nombre_madre.map(lambda x: len(x.split()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf['n_char_nombre'] = nf.nombre.map(len)\n",
    "nf['n_char_prenames'] = nf.prenames.map(len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "bins = np.arange(80)\n",
    "ax.set(yscale='log')\n",
    "\n",
    "ax.hist(nf.n_char_nombre, bins=bins, color='blue', alpha=0.3);\n",
    "ax.hist(nf.n_char_prenames, bins=bins, color='red', alpha=0.3);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf[nf.n_char_nombre < 10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use surname/prename frequency to adjust results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parse_prename(prenames):\n",
    "    \"\"\" The surnames are parsed, but the prenames must be split up.  \n",
    "    This is possible once the multi-part prenames have been given underscores \n",
    "    \"\"\"\n",
    "    \n",
    "    out = {'pre1':\"\", 'pre2':\"\", 'pre3':\"\", 'junk':\"\"}\n",
    "    \n",
    "    # now assign name pices\n",
    "    pres = prenames.split()\n",
    "    if len(pres) >= 1:\n",
    "        out['pre1'] = pres[0]\n",
    "    if len(pres) >= 2:\n",
    "        out['pre2'] = pres[1]\n",
    "    if len(pres) >= 3:\n",
    "        out['pre3'] = pres[2]\n",
    "    if len(pres) >= 4:\n",
    "        out['junk'] = ' '.join(pres[3:])\n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def count_all_names(chk):\n",
    "    tmp = pd.concat([chk.sur_padre, chk.sur_madre], axis=0).value_counts()\n",
    "    count_sur = pd.DataFrame({'obsname':tmp.index, 'n_sur':tmp.values})\n",
    "    tmp = pd.concat([chk.pre1, chk.pre2], axis=0).value_counts()\n",
    "    count_pre = pd.DataFrame({'obsname':tmp.index, 'n_pre':tmp.values})\n",
    "\n",
    "    count_names = count_sur.merge(count_pre, on='obsname', how='outer')\n",
    "    count_names.fillna(0, inplace=True)\n",
    "\n",
    "    # add null record, so that null names get weight factor of 1\n",
    "    count_names.loc[count_names.obsname == \"\", ['n_sur','n_pre']] = 0\n",
    "\n",
    "    count_names['n_sur'] = count_names.n_sur + 0.5\n",
    "    count_names['n_pre'] = count_names.n_pre + 0.5\n",
    "\n",
    "    count_names['sratio'] = count_names.n_sur / count_names.n_pre\n",
    "    count_names['pratio'] = count_names.n_pre / count_names.n_sur\n",
    "    \n",
    "    return count_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def is_name_multimatch(nombre):\n",
    "    mdel   = re_del.search(nombre)\n",
    "    msant  = re_sant.search(nombre)\n",
    "    \n",
    "    mlaos  = re_laos.search(nombre)\n",
    "    mdela  = re_dela.search(nombre)\n",
    "    \n",
    "    mde  = re_de.search(nombre)\n",
    "    \n",
    "    mvon   = re_von.search(nombre)\n",
    "    mvande = re_vande.search(nombre)\n",
    "    \n",
    "    if mdel or msant or mlaos or mdela or mde or mvon or mvande:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "if full_run:\n",
    "    chk = pd.concat([nf[['cedula', 'nombre', 'sur_padre', 'sur_madre']], \n",
    "                     nf.progress_apply(lambda row: parse_prename(row.prenames), axis=1, result_type='expand')], axis=1)\n",
    "\n",
    "    chk['nlen'] = chk[['sur_padre','sur_madre','pre1','pre2','pre3','junk']\n",
    "                     ].replace(\"\", np.nan).notnull().astype(int).sum(axis=1)\n",
    "    print(\"saving...\")\n",
    "    chk.to_csv(\"CHKFILE_\" + TODAY + \".tsv\", sep='\\t', index=False)\n",
    "else:\n",
    "    chk = pd.read_csv(\"CHKFILE_\" + READ_DATE + \".tsv\", sep='\\t', dtype=str)\n",
    "\n",
    "print(\"# recs :\", len(chk))\n",
    "\n",
    "# 90 mins"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "count_names = count_all_names(chk)\n",
    "count_names['nlen'] = count_names.obsname.map(lambda x: len(x.split()))\n",
    "count_names['is_multimatch'] = count_names.obsname.map(is_name_multimatch)\n",
    "\n",
    "if full_run:\n",
    "    count_names.to_csv(\"NAMECOUNTS_\" + TODAY + \".tsv\", sep='\\t', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check single-token surnames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "solo_sur = nf[nf.sur_padre.map(lambda x: len(x.split()) == 1)]\n",
    "solo_counts = solo_sur.sur_padre.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "bins = np.arange(0,100000,100)\n",
    "ax.set(yscale='log')\n",
    "ax.hist(solo_counts, bins=bins);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "solo_counts[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "surs_solo_rare = set(solo_counts[solo_counts < 10].index)\n",
    "len(surs_solo_rare)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# how many records are used by \"rare\" surnames?\n",
    "nf.sur_padre.isin(surs_solo_rare).sum()   # 101k (out of 20M), so 0.5%"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the two-token surnames, many are actually a sur-pre combo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dual_sur = count_names[(count_names.nlen == 2) & ~count_names.is_multimatch]\n",
    "dual_sur = dual_sur.apply(lambda x: x.obsname.split(), axis=1, result_type='expand')\n",
    "dual_sur.columns = ['probably_sur', 'probably_pre']\n",
    "\n",
    "dual_sur = dual_sur.merge(count_names[['obsname','sratio']], \n",
    "                left_on='probably_sur', right_on='obsname').drop(columns=['obsname'])\n",
    "\n",
    "dual_sur = dual_sur.merge(count_names[['obsname','pratio']], \n",
    "                left_on='probably_pre', right_on='obsname').drop(columns=['obsname'])\n",
    "\n",
    "dual_sur['evidence'] = dual_sur.sratio * dual_sur.pratio\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dual_sur"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_samp = np.min((30, len(dual_sur[dual_sur.evidence < 100])))\n",
    "dual_sur[(dual_sur.evidence < 100) ].sample(n_samp) # & (tmp.probably_pre != \"MARIA\")]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dual_sur[dual_sur.probably_pre == 'MARIA']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "ax.set(xscale='log', xlabel='evidence name is actually sur+pre', ylabel='frequency')\n",
    "\n",
    "logbins = np.logspace(-6,10,129)\n",
    "ax.hist(dual_sur.evidence, bins=logbins, color='blue', alpha=0.3);\n",
    "ax.hist(dual_sur[dual_sur.probably_pre == 'MARIA'].evidence, bins=logbins, color='red', alpha=0.3);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub = dual_sur[dual_sur.probably_pre.isin({'MARIA','JORGE','CARMEN','JOSE'})]\n",
    "\n",
    "ax.hist(sub.evidence, bins=logbins, color='red', alpha=0.3);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dual_sur[(dual_sur.evidence < 20) & (dual_sur.probably_pre == 'MARIA')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dual_sur[dual_sur.evidence > 100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ~ 120k sur_madre need this treatment.  But only ~300 padres (and most of those seem to actually be correct)\n",
    "needs_repair = dual_sur[dual_sur.evidence > 1000]\n",
    "needs_repair = set(needs_repair.probably_sur + ' ' + needs_repair.probably_pre)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def repair_dual_surmadre(row):\n",
    "    out = {'sur_madre':\"\", 'prenames':\"\"}\n",
    "    sur_madre, pre1 = row.sur_madre.split()\n",
    "    \n",
    "    out['prenames'] = pre1 + ' ' + row.prenames\n",
    "    out['sur_madre'] = sur_madre\n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf.loc[nf.sur_madre.isin(needs_repair), ['sur_madre','prenames']\n",
    "      ] = nf[nf.sur_madre.isin(needs_repair)].progress_apply(lambda row: repair_dual_surmadre(row), axis=1, result_type='expand')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Following cleanup, re-check the prename parsing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del chk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "if full_run:\n",
    "    nf.to_csv('names_cleaned_' + TODAY + '.tsv', sep='\\t', index=False)\n",
    "\n",
    "if False:\n",
    "    nf.to_csv('names_cleaned_' + READ_DATE + '.tsv', sep='\\t', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "if full_run:\n",
    "    \n",
    "    newchk = pd.concat([nf[['cedula', 'nombre', 'sur_padre', 'sur_madre']], \n",
    "                     nf.progress_apply(lambda row: parse_prename(row.prenames), axis=1, result_type='expand')], axis=1)\n",
    "\n",
    "    newchk['nlen'] = newchk[['sur_padre','sur_madre','pre1','pre2','pre3','junk']\n",
    "                     ].replace(\"\", np.nan).notnull().astype(int).sum(axis=1)\n",
    "    print(\"saving...\")\n",
    "    newchk.to_csv(\"NEWCHKFILE_\" + TODAY + \".tsv\", sep='\\t', index=False)\n",
    "else:\n",
    "    newchk = pd.read_csv(\"NEWCHKFILE_\" + READ_DATE + \".tsv\", sep='\\t', dtype=str)\n",
    "\n",
    "print(\"# recs :\", len(newchk))\n",
    "\n",
    "# 90 mins"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "count_names = count_all_names(newchk)\n",
    "count_names['nlen'] = count_names.obsname.map(lambda x: len(x.split()))\n",
    "count_names['is_multimatch'] = count_names.obsname.map(is_name_multimatch)\n",
    "\n",
    "if full_run:\n",
    "    count_names.to_csv(\"NAMECOUNTS_\" + TODAY + \".tsv\", sep='\\t', index=False)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spousal names\n",
    "\n",
    "Here I'm assuming the husband's name is only one token; should revisit at some point"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "# cleans mother's name of the \"DE HUSBAND\" junk\n",
    "re_despouse = re.compile(u\"\\sDE (\\w{2,})(\\s|$)\")\n",
    "\n",
    "def clean_spousename(row):\n",
    "    m = re_despouse.search(row.nombre_madre)\n",
    "    \n",
    "    # group1 might be husband's name, compare to already-extracted husband name\n",
    "    if m and (m.group(1) in row.nombre_padre):\n",
    "        # if it matches, replace with space (and strip, to avoid leaving trailing space)\n",
    "              \n",
    "        # NB : improve this by finding the longest husband-name that matches (as in the parse_fullrow alg)\n",
    "        # ALSO, re-match the child/mother name\n",
    "        return (re_despouse.sub(\" \", row.nombre_madre)).strip()\n",
    "    else:\n",
    "        return row.nombre_madre\n",
    "\n",
    "clean_mom = out.progress_apply(lambda row: clean_spousename(row), axis=1)\n",
    "out['nombre_madre'] = clean_mom"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fname = \"cleaned_parsed_names_\" + TODAY + \".tsv\"\n",
    "out.to_csv(fname, sep='\\t', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identify and fix funky names (\"VAN DER HOOK\", \"DE LA CRUZ\", etc)\n",
    "\n",
    "This takes a while to run, bc ~N**2 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# in all cases, we look for a word boundary as the first group, then our funky name as the second\n",
    "re_von = re.compile(u\"(^|\\s)(V[AO]N \\w{2,})(\\s|$)\")              # these results are subset of \"re_vande\"\n",
    "re_vande = re.compile(u\"(^|\\s)(V[AO]N DE[RN]? \\w{2,})(\\s|$)\")\n",
    "re_sant = re.compile(u\"(^|\\s)(SANT?A? \\w{2,})(\\s|$)\")\n",
    "re_dela = re.compile(u\"(^|\\s)(DE L[AO]S? [AO]? ?\\w{2,})(\\s|$)\")   # these results are subset of \"re_laos\"\n",
    "re_laos = re.compile(u\"(^|\\s)(L[AEO]S? \\w{2,})(\\s|$)\")\n",
    "re_del  = re.compile(u\"(^|\\s)(DEL \\w{2,})(\\s|$)\")\n",
    "\n",
    "\n",
    "def regex_funky(row):\n",
    "    \"\"\" This is a little slow (~4mins / million rows), but pretty thorough.  \"\"\"\n",
    "    \n",
    "    mdel   = re_del.search(row.nombre)\n",
    "    msant  = re_sant.search(row.nombre)\n",
    "    \n",
    "    mlaos  = re_laos.search(row.nombre)\n",
    "    mdela  = re_dela.search(row.nombre)\n",
    "    \n",
    "    mvon   = re_von.search(row.nombre)\n",
    "    mvande = re_vande.search(row.nombre)\n",
    "    \n",
    "    poss_funks = set()\n",
    "    \n",
    "    if mdel:\n",
    "        poss_funks.add(mdel.group(2))\n",
    "    if msant:\n",
    "        poss_funks.add(msant.group(2))\n",
    "    if mvon:\n",
    "        # \"VAN DE\" types are a subset of \"VAN\" types\n",
    "        if mvande:\n",
    "            poss_funks.add(mvande.group(2))\n",
    "        else:\n",
    "            poss_funks.add(mvon.group(2))\n",
    "    if mlaos:\n",
    "        # \"DE LA\" type names are a subset of \"LA\" types\n",
    "        if mdela:\n",
    "            poss_funks.add(mdela.group(2))\n",
    "        else:\n",
    "            poss_funks.add(mlaos.group(2))\n",
    "    \n",
    "    for poss_funk in poss_funks:\n",
    "        \n",
    "        if row.nombre.endswith(poss_funk):\n",
    "            # assume it's a prename\n",
    "            funky_prenames.append(poss_funk)\n",
    "            \n",
    "        elif row.nombre_padre and (poss_funk in row.nombre_padre):\n",
    "            # father's surname\n",
    "            funky_surnames.append(poss_funk)\n",
    "            \n",
    "        elif row.nombre_madre and (poss_funk in row.nombre_madre):\n",
    "            # mother's surname\n",
    "            funky_surnames.append(poss_funk)\n",
    "            \n",
    "        elif not row.nombre_padre and not row.nombre_madre:\n",
    "            # no parent data available, so just assume it's correct\n",
    "            funky_surnames.append(poss_funk)\n",
    "\n",
    "        else:\n",
    "            # something's fucky (possible misspelling ?)\n",
    "            funky_junk.append(poss_funk)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "sub = rf.sample(1000000)\n",
    "\n",
    "funky_surnames = list()\n",
    "funky_prenames = list()\n",
    "funky_junk = list()\n",
    "_ = sub.apply(lambda row: regex_funky(row), axis=1)\n",
    "\n",
    "funkcount = Counter(funky_junk)\n",
    "surcount = Counter(funky_surnames)\n",
    "precount = Counter(funky_prenames)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_weirds(row, multi, df, nameset):\n",
    "    \n",
    "    MIN_RECS = 2\n",
    "    new_names = set()\n",
    "    parts = [x.strip() for x in row.nombre.split(multi)[0].split()]\n",
    "    for ind in np.arange(len(parts)):\n",
    "        guess = ' '.join(parts[ind:] + [multi])\n",
    "        if guess in nameset:\n",
    "            # it's already known to be a last nome, just skip\n",
    "            break\n",
    "            \n",
    "        # don't draw conclusions unless we have data from both parents\n",
    "        if (row.nombre_padre and row.nombre_madre):\n",
    "            if (   (guess in row.nombre_padre and not multi in row.nombre_madre)\n",
    "                or (guess in row.nombre_madre and not multi in row.nombre_padre) ):\n",
    "                if df.nombre.map(lambda x: x.startswith(guess)).sum() >= MIN_RECS:\n",
    "                    print(\"\\t\", guess)\n",
    "                    new_names.add(guess)\n",
    "        \n",
    "        \"\"\"\n",
    "                if (guess in row.nombre_padre) or (guess in row.nombre_madre):\n",
    "            if df.nombre.map(lambda x: x.startswith(guess)).sum() >= MIN_RECS:\n",
    "                print(\"\\t\", guess)\n",
    "                new_names.add(guess)\n",
    "        \"\"\"\n",
    "    return new_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "nameset = set(surcount)\n",
    "\n",
    "# funky/multi-word surnames which have occurred more than once\n",
    "multis = [x for x,y in surcount.items() if y > 1]\n",
    "for multi in multis:\n",
    "    \n",
    "    # check how often the proposed surname is the first element/surname  \n",
    "    # If that happens rarely, then there might be another word in front\n",
    "    sub = rf[rf.nombre.map(lambda x: x.startswith(multi))]\n",
    "    thresh = np.max([2, np.floor(np.sqrt(surcount[multi]))])\n",
    "    if len(sub) <= thresh:\n",
    "        print(multi)\n",
    "        for ind, row in rf[rf.nombre.map(lambda x: multi in x)].iterrows():\n",
    "            new_names = check_weirds(row, multi, rf, nameset)\n",
    "            nameset = nameset | new_names\n",
    "\n",
    "#  ~ 5 mins/million"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "junky = list()\n",
    "my_ynames = list()\n",
    "\n",
    "def get_ynames(row):\n",
    "    names = row.nombre.split()\n",
    "    ind_y = names.index('Y')\n",
    "    \n",
    "    pre = names[ind_y - 1]\n",
    "    post = names[ind_y + 1]\n",
    "    \n",
    "    if pre == post:\n",
    "        # sometimes \"SANCHEZ Y SANCHEZ\" is used to indicate that both parents had the same name.  \n",
    "        pass\n",
    "    else:\n",
    "        possible = pre + ' Y ' + post\n",
    "\n",
    "        if possible in row.nombre_padre or possible in row.nombre_madre:\n",
    "#            funky_surnames.append(possible)\n",
    "            my_ynames.append(possible)\n",
    "        else:\n",
    "            junky.append(possible)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "has_y = rf[rf.nombre.map(lambda x: \" Y \" in x)]\n",
    "has_y.apply(lambda row: get_ynames(row), axis=1);\n",
    "\n",
    "junk_count = Counter(junky)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Counter(my_ynames)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "funky_surnames = list(set(surcount) | set(my_ynames) | nameset)\n",
    "funky_surnames.sort(reverse=True, key=len)\n",
    "len(funky_surnames)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re_laroy = re.compile(r\"(^)LA ROSA(\\s|$)\")\n",
    "rf[rf.nombre.map(lambda x: True if re_laroy.search(x) else False)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "funky_prenames = list(set(precount))\n",
    "funky_prenames.sort(reverse=True, key=len)\n",
    "len(funky_prenames)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "funks = funky_surnames + funky_prenames\n",
    "funks.sort(reverse=True, key=len)\n",
    "funks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "def fix_funk(nombre, funks):\n",
    "    \"\"\" The 'funks' list should be sorted in descending length, to prevent substrings from being clobbered.\"\"\"\n",
    "    nlen = len(nombre.split())\n",
    "    for funk in funks:\n",
    "        flen = len(funk.split())\n",
    "        if (nlen - flen > 1):\n",
    "            if (funk in nombre):\n",
    "                defunk = '_'.join(funk.split())\n",
    "                nombre = defunk.join(nombre.split(funk))\n",
    "                nlen = len(nombre.split())\n",
    "        else:\n",
    "            # since the list is sorted, once we have a match that uses all the tokens, just skip ahead\n",
    "            continue\n",
    "    return nombre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "def fix_funk(nombre, funks):\n",
    "    \"\"\" The 'funks' list should be sorted in descending length, to prevent substrings from being clobbered.\"\"\"\n",
    "    nlen = len(nombre.split())\n",
    "    for funk in funks:\n",
    "        flen = len(funk.split())\n",
    "        if (nlen - flen > 1):\n",
    "            if (funk in nombre):\n",
    "                defunk = '_'.join(funk.split())\n",
    "                nombre = defunk.join(nombre.split(funk))\n",
    "                nlen = len(nombre.split())\n",
    "        else:\n",
    "            # since the list is sorted, once we have a match that uses all the tokens, just skip ahead\n",
    "            continue\n",
    "    return nombre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import namedtuple"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re_under = re.compile(r\"(^|\\s)(\\w+_\\w*)(\\s|$)\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re_under.findall(\"DE_LA_CRUZ SOME WAN\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "help(re_under.finditer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(rf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "parsed = rf.nombre.map(lambda x: fix_funk(x, funks))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parsed[rf.nombre.map(lambda x: \"DE LA CRUZ\" in x)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split names, write output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def write_nameparts(nombre):\n",
    "    \n",
    "    rec = {'sur1': '',\n",
    "           'sur2': '',\n",
    "           'pre1': '',\n",
    "           'pre2': '',\n",
    "           'pre3': '',\n",
    "           'junk': ''\n",
    "          }\n",
    "    \n",
    "    try:\n",
    "        tokens = nombre.split()\n",
    "    except AttributeError:\n",
    "        # when something besids a string is passed in\n",
    "        tokens = []\n",
    "    \n",
    "    # get rid of rogue commas\n",
    "    if ',' in nombre:\n",
    "        print(nombre)\n",
    "        nombre = ' '.join(nombre.split(','))\n",
    "        \n",
    "    tlen = len(tokens)\n",
    "    \n",
    "    if tlen < 2:\n",
    "        return pd.Series(rec)\n",
    "    \n",
    "    elif tlen == 2:\n",
    "        rec['sur1'] = tokens[0]\n",
    "        rec['pre1'] = tokens[1]\n",
    "        \n",
    "    elif tlen == 3:\n",
    "        # assume only 1 surname - it's correct for extranjeros, and ecudoranos will get fixed by the algorithm\n",
    "        rec['sur1'] = tokens[0]\n",
    "        rec['pre1'] = tokens[1]\n",
    "        rec['pre2'] = tokens[2]\n",
    "        \n",
    "    else :\n",
    "        rec['sur1'] = tokens[0]\n",
    "        rec['sur2'] = tokens[1]\n",
    "        rec['pre1'] = tokens[2]\n",
    "        \n",
    "        if tlen > 4:\n",
    "            rec['pre3'] = tokens[4]\n",
    "        if tlen > 5:\n",
    "            rec['junk'] = ' '.join(tokens[5:])\n",
    "    return pd.Series(rec)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parse_column(rf, col):\n",
    "    namecols = ['sur1', 'sur2', 'pre1', 'pre2', 'pre3', 'junk']\n",
    "    \n",
    "    now = dt.datetime.now()\n",
    "    fname = col + \"_\" + now.strftime(\"%Y%m%dT%H%M\") + \".tsv\"\n",
    "    \n",
    "    datacol = rf[col]\n",
    "    cedulas = rf['cedula']\n",
    "    nlens = datacol.map(lambda x: len(x.split())).astype(str)\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('\\t'.join(['cedula','nombre','nlen'] + namecols) + '\\n')\n",
    "        for ind, nombre in tqdm(datacol.items()):\n",
    "            vals = write_nameparts(nombre)\n",
    "            f.write('\\t'.join([cedulas[ind], nombre, nlens[ind]] + [v for v in vals]) + '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "parse_column(rf, 'nombre')\n",
    "\n",
    "parse_column(rf, 'nombre_padre')\n",
    "\n",
    "parse_column(rf, 'nombre_madre')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i, target in datacol.items():\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datacol = rf['nombre']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for namefile in namefiles:\n",
    "    tokens = namefile.split(\"/\")\n",
    "    path = \"/\".join(tokens[:-1])\n",
    "    nameout = tokens[-1][:-4] + '_PARSED.tsv'\n",
    "    print(nameout)\n",
    "    \n",
    "    df = load_large_dta(namefile)\n",
    "    df.rename(columns={'name':'nombre', 'name_len':'nlen'}, inplace=True)\n",
    "    df['nlen'] = df.nlen.astype(str)\n",
    "\n",
    "    with open(nameout, 'w') as f:\n",
    "        f.write('\\t'.join(['cc','nombre','nlen'] + namecols) + '\\n')\n",
    "        for ind, row in tqdm(df.iterrows()):\n",
    "            vals = write_nameparts(row)\n",
    "            f.write('\\t'.join([row.cc, row.nombre, row.nlen] + [v for v in vals]) + '\\n')\n",
    "    \n",
    "    if i == 0:\n",
    "        ff = df.copy(deep=True)\n",
    "    i += 1"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}