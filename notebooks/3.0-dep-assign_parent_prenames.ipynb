{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assign parent prenames\n",
    "\n",
    "We've already identified the \"first surname\" (i.e. father's surname) for each parent, via comparison to the citizen. Now we inspect the parent's name more closely, and try to determine which tokens are prenames vs additional surname.  \n",
    "We take advantage of knowing how often names are used as prenames or surnames with the citizens in general. Basically we ask for each token \"if this were a prename (surname), would this name be more likely overall\". I'm pretty sure it's a MAXENT kinda thing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import datetime as dt\n",
    "\n",
    "import unidecode\n",
    "from fuzzywuzzy import fuzz    \n",
    "    \n",
    "# enable progress bar on long operations\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_run = True\n",
    "N_ROWS = None # 1000000\n",
    "READ_DATE = '20200824'\n",
    "READ_DATE = '20201026'\n",
    "READ_DATE = '20201111'\n",
    "\n",
    "\n",
    "LOC_RAW = \"../data/raw/\"\n",
    "LOC_INTERIM = \"../data/interim/\"\n",
    "\n",
    "TODAY = dt.datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "TODAY = READ_DATE\n",
    "TODAY"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "nan_values = ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', # 'NA' is sometimes name\n",
    "              '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', '']\n",
    "\n",
    "\n",
    "dtypes_reg = {  \"cedula\":   str,\n",
    "                \"nombre\":   str,\n",
    "                \"gender\":  'category',\n",
    "                \"dt_birth\": str,\n",
    "                \"nationality\":   'category',\n",
    "                \"nombre_spouse\":  str,\n",
    "                \"dt_death\":       str,\n",
    "                \"orig_cedula\":    str,\n",
    "                \"marital_status\": 'category',\n",
    "                \"nombre_padre\":\tstr,\n",
    "                \"nombre_madre\":\tstr,\n",
    "                \"ced_spouse\":\tstr,\n",
    "                \"ced_padre\":\tstr,\n",
    "                \"ced_madre\":\tstr,\n",
    "                \"dt_marriage\":\tstr\n",
    "                }\n",
    "\n",
    "usecols_reg = ['cedula','nombre','gender','nombre_spouse','nombre_padre','nombre_madre']\n",
    "\n",
    "# ~80 sec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "dtypes_names = {'cedula':str, 'sur_padre':str, 'sur_madre':str, 'prenames':str,\n",
    "               'has_padre':bool, 'has_madre':bool, 'is_plegal':'category', 'is_mlegal':'category'\n",
    "               }\n",
    "nf = pd.read_csv(LOC_INTERIM + \"names_cleaned_\" + READ_DATE + \".tsv\", sep='\\t',\n",
    "                 dtype=dtypes_names,\n",
    "                 keep_default_na=False, na_values=nan_values,\n",
    "                nrows=N_ROWS\n",
    "                )\n",
    "print(\"# NF recs :\", len(nf))\n",
    "\n",
    "if True:\n",
    "    nf.loc[nf.sur_padre.isnull(), 'sur_padre'] = \"\"\n",
    "    nf.loc[nf.sur_madre.isnull(), 'sur_madre'] = \"\"\n",
    "    nf.loc[nf.prenames.isnull(), 'prenames'] = \"\"\n",
    "    nf['nlen_pre'] = nf.prenames.map(lambda x: len(x.split()))\n",
    "    nf['is_plegal'] = nf.is_plegal.map(lambda x: np.nan if x is np.nan else bool(x))\n",
    "    nf['is_mlegal'] = nf.is_mlegal.map(lambda x: np.nan if x is np.nan else bool(x))\n",
    "\n",
    "\n",
    "# ~90 sec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "nf.drop(['n_char_nombre','n_char_prenames', 'nlen_pre'], axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(nf.has_padre & nf.has_madre & (nf.sur_padre != \"\") & (nf.sur_madre != \"\")).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf.is_plegal.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load prenames from regular data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "file_freq = LOC_INTERIM + \"NEWFREQFILE_\" + READ_DATE + \".tsv\"\n",
    "\n",
    "freq = pd.read_csv(file_freq, sep='\\t', dtype=str)\n",
    "len(freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def count_all_names(freq):\n",
    "    tmp = pd.concat([freq.sur_padre, freq.sur_madre], axis=0).value_counts()\n",
    "    count_sur = pd.DataFrame({'obsname':tmp.index, 'n_sur':tmp.values})\n",
    "    tmp = pd.concat([freq.pre1, freq.pre2], axis=0).value_counts()\n",
    "    count_pre = pd.DataFrame({'obsname':tmp.index, 'n_pre':tmp.values})\n",
    "\n",
    "    count_names = count_sur.merge(count_pre, on='obsname', how='outer')\n",
    "    count_names.fillna(0, inplace=True)\n",
    "\n",
    "    # add null record, so that null names get weight factor of 1\n",
    "    count_names.loc[count_names.obsname == \"\", ['n_sur','n_pre']] = 0\n",
    "\n",
    "    count_names['n_sur'] = count_names.n_sur + 0.5\n",
    "    count_names['n_pre'] = count_names.n_pre + 0.5\n",
    "\n",
    "    count_names['sratio'] = count_names.n_sur / count_names.n_pre\n",
    "    count_names['pratio'] = count_names.n_pre / count_names.n_sur\n",
    "    \n",
    "    return count_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "ncounts = count_all_names(freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ncounts[:20]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ncounts[ncounts.obsname.map(lambda x: len(x.split()) > 1)][-50:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%time\n",
    "\n",
    "def merge_underscore_names(ncounts):\n",
    "    under_prenames = set(ncounts[ncounts.obsname.map(lambda x: \"_\" in x)].obsname)\n",
    "\n",
    "    for upre in tqdm(under_prenames):\n",
    "\n",
    "        u_rec = ncounts[ncounts.obsname == upre].iloc[0]\n",
    "\n",
    "        norm_pre = ' '.join(upre.split(\"_\"))\n",
    "        norm_rec = ncounts[ncounts.obsname == norm_pre]\n",
    "        if len(norm_rec) == 1:\n",
    "            norm_rec = norm_rec.iloc[0]\n",
    "            ncounts.loc[ncounts.obsname == norm_pre, 'n_sur'] = u_rec.n_sur + norm_rec.n_sur - 0.5\n",
    "            ncounts.loc[ncounts.obsname == norm_pre, 'n_pre'] = u_rec.n_pre + norm_rec.n_pre - 0.5\n",
    "        elif len(norm_rec) == 0:\n",
    "            tmp = u_rec.copy(deep=True)\n",
    "            tmp.obsname = norm_pre\n",
    "            ncounts = ncounts.append(tmp)\n",
    "\n",
    "    ncounts = ncounts[~ncounts.obsname.isin(under_prenames)]\n",
    "    ncounts['sratio'] = ncounts.n_sur/ncounts.n_pre\n",
    "    ncounts['pratio'] = ncounts.n_pre/ncounts.n_sur\n",
    "    \n",
    "    subspace = ncounts[ncounts.obsname.map(lambda x: \" \" in x)].copy(deep=True)\n",
    "    subspace['obsname'] = subspace.obsname.map(lambda x: \"_\".join(x.split()))\n",
    "    return pd.concat([ncounts, subspace], axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "allnames = pd.read_csv(\"../data/interim/ALLNAMES_\" + READ_DATE + \".tsv\", sep='\\t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "allnames[allnames.obsname == 'RODRIGUEZ']  # use as prename looks legit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Identify \"multinames\" for madres/padres"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "funky_prenames = set()\n",
    "with open(LOC_INTERIM + \"funky_prenames_\" + READ_DATE + \".txt\") as f:\n",
    "    for line in f:\n",
    "        funky_prenames.add(line.strip())\n",
    "\n",
    "funky_prenames = list(funky_prenames)\n",
    "funky_prenames.sort(reverse=True, key=len)\n",
    "print(\"# funkies :\", len(funky_prenames))\n",
    "\n",
    "def fix_funk(nombre, funks):\n",
    "    \"\"\" The 'funks' list should be sorted in descending length, to prevent substrings from being clobbered.\n",
    "    \n",
    "    NB: there's a potential bug in here, bc the list is sorted according to character length, but checks\n",
    "    here are being done according to number of tokens.  But very unlikely to cause an issue, so ignoring for now\n",
    "    \"\"\"\n",
    "    nlen = len(nombre.split())\n",
    "    for funk in funks:\n",
    "        flen = len(funk.split())\n",
    "        if (nlen > flen):\n",
    "            if (funk in nombre):\n",
    "                defunk = '_'.join(funk.split())\n",
    "                nombre = defunk.join(nombre.split(funk))\n",
    "                nlen = len(nombre.split())\n",
    "        else:\n",
    "            # since the list is sorted, once we have a match that uses all the tokens, just skip ahead\n",
    "            continue\n",
    "    return nombre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def parse_pres(pres, pname, row=None):\n",
    "\n",
    "    if len(pres.split()) == 1:\n",
    "        pname['pre1'] = pres\n",
    "    elif len(pres.split()) == 2:\n",
    "        pname['pre1'],pname['pre2'] = pres.split()\n",
    "    else:\n",
    "        pres = fix_funk(pres, funky_prenames).split()\n",
    "        if len(pres) == 0:\n",
    "            print(\"WTF at {0} - PRES: {1} PNAME: {2}\".format(row.cedula, pres, pname))\n",
    "            pass\n",
    "        else:\n",
    "            pname['pre1'] = pres[0]\n",
    "            if len(pres) > 1:\n",
    "                pname['pre2'] = pres[1]\n",
    "                if len(pres) > 2:\n",
    "                    pname['pre3'] = pres[2]\n",
    "                    if len(pres) > 3:\n",
    "                        pname['junk'] = ' '.join(pres[4:])\n",
    "    return pname"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def assign_pres(pres, pname):\n",
    "    pname['pre1'] = pres[0]\n",
    "    if len(pres) > 1:\n",
    "        pname['pre2'] = pres[1]\n",
    "        if len(pres) > 2:\n",
    "            pname['pre3'] = pres[2]\n",
    "            if len(pres) > 3:\n",
    "                pname['junk'] = ' '.join(pres[4:])\n",
    "    return pname"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "# this uses dict lookup, rather than dataframe lookup.  Faster by ~100x\n",
    "\n",
    "wts_pre = dict()\n",
    "wts_sur = dict()\n",
    "\n",
    "for _, row in allnames.iterrows():\n",
    "    wts_pre[row.obsname] = row.pratio\n",
    "    wts_sur[row.obsname] = row.sratio\n",
    "    \n",
    "    \n",
    "def calc_evidence(pres, surs, wts_pre, wts_sur):\n",
    "    \"\"\" Calculate the evidence for a particular configuration of prename/surname allocation. \"\"\"\n",
    "    evidence = 1\n",
    "    for pre in pres:\n",
    "        try:\n",
    "            wt_pre = wts_pre[pre]\n",
    "        except KeyError:\n",
    "            wt_pre = 1\n",
    "        evidence = evidence * wt_pre\n",
    "        \n",
    "    for sur in surs:\n",
    "        try:\n",
    "            wt_sur = wts_sur[sur]\n",
    "        except KeyError:\n",
    "            wt_sur = 1\n",
    "        evidence = evidence * wt_sur\n",
    "    return evidence\n",
    "\n",
    "# 2 minutes to load"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# we'll only need these to check names at the start of a string\n",
    "re_beg_von = re.compile(u\"^(V[AO]N \\w{2,})(\\s|$)\")                # these results are subset of \"re_beg_vande\"\n",
    "re_beg_vande = re.compile(u\"^(V[AO]N DE[RN]? \\w{2,})(\\s|$)\")\n",
    "re_beg_sant = re.compile(u\"^(SANT?A? \\w{2,})(\\s|$)\")              # SANTA and SAN (in lieu of SANTO)\n",
    "re_beg_dela = re.compile(u\"^(DE L[AO]S? ?\\w{2,})(\\s|$)\")   # these results are subset of \"re_beg_laos\"\n",
    "re_beg_laos = re.compile(u\"^(L[AEO]S? \\w{2,})(\\s|$)\")\n",
    "re_beg_del  = re.compile(u\"^(DEL \\w{2,})(\\s|$)\")\n",
    "re_beg_de   = re.compile(r\"^(DE \\w{2,})(\\s|$)\")\n",
    "\n",
    "\n",
    "def get_starting_multimatch(nombre):\n",
    "    \"\"\" Check if the beginning of the string is a multiname. If so, return the multiname.\"\"\"\n",
    "    \n",
    "    mdela  = re_beg_dela.match(nombre)\n",
    "    if mdela:\n",
    "        return mdela.group(1)\n",
    "    \n",
    "    mlaos  = re_beg_laos.match(nombre)\n",
    "    if mlaos:\n",
    "        return mlaos.group(1)\n",
    "    \n",
    "    mde    = re_beg_de.match(nombre)\n",
    "    if mde:\n",
    "        return mde.group(1)\n",
    "    \n",
    "    mdel   = re_beg_del.match(nombre)\n",
    "    if mdel:\n",
    "        return mdel.group(1)\n",
    "    \n",
    "    mvande = re_beg_vande.match(nombre)\n",
    "    if mvande:\n",
    "        return mvande.group(1)\n",
    "    \n",
    "    mvon   = re_beg_von.match(nombre)\n",
    "    if mvon:\n",
    "        return mvon.group(1)\n",
    "    \n",
    "    return \"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# we'll only need these to check names at the end of a string\n",
    "re_end_von = re.compile(u\".*\\s(V[AO]N \\w{2,})$\")                # these results are subset of \"re_end_vande\"\n",
    "re_end_vande = re.compile(u\".*\\s(V[AO]N DE[RN]? \\w{2,})$\")\n",
    "re_end_sant = re.compile(u\".*\\s(SANT?A? \\w{2,})$\")              # SANTA and SAN (in lieu of SANTO)\n",
    "re_end_dela = re.compile(u\".*\\s(DE L[AO]S? ?\\w{2,})$\")   # these results are subset of \"re_end_laos\"\n",
    "re_end_laos = re.compile(u\".*\\s(L[AEO]S? \\w{2,})$\")\n",
    "re_end_del  = re.compile(u\".*\\s(DEL \\w{2,})$\")\n",
    "re_end_de   = re.compile(r\".*\\s(DE \\w{2,})$\")\n",
    "\n",
    "\n",
    "def get_ending_multimatch(nombre):\n",
    "    \"\"\" Check if the end of the string is a multiname. If so, return the multiname.\"\"\"\n",
    "    \n",
    "    mdela  = re_end_dela.match(nombre)\n",
    "    if mdela:\n",
    "        return mdela.group(1)\n",
    "    \n",
    "    mlaos  = re_end_laos.match(nombre)\n",
    "    if mlaos:\n",
    "        return mlaos.group(1)\n",
    "    \n",
    "    mde    = re_end_de.match(nombre)\n",
    "    if mde:\n",
    "        return mde.group(1)\n",
    "    \n",
    "    mdel   = re_end_del.match(nombre)\n",
    "    if mdel:\n",
    "        return mdel.group(1)\n",
    "    \n",
    "    mvande = re_end_vande.match(nombre)\n",
    "    if mvande:\n",
    "        return mvande.group(1)\n",
    "    \n",
    "    mvon   = re_end_von.match(nombre)\n",
    "    if mvon:\n",
    "        return mvon.group(1)\n",
    "    \n",
    "    return \"\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def best_split_by_evidence(my_pres, row, verbose=False):\n",
    "    #     As is, this function flags around 1 name per 1000 as having issues.\n",
    "    flag_split = False\n",
    "    best_split = None    # setting a default, for weird cases I didn't think of\n",
    "    evids = []\n",
    "    for ind in range(len(my_pres)):\n",
    "        surs = my_pres[:ind]\n",
    "        pres = my_pres[ind:]\n",
    "        evids.append(calc_evidence(pres, surs, wts_pre, wts_sur))\n",
    "\n",
    "    if len(evids) > 0:\n",
    "        best_split = np.argmax(evids)\n",
    "        if (best_split > 1):\n",
    "            flag_split = True\n",
    "            if verbose:\n",
    "                print(\"WARNING at ced {0} - split at {1} implies three surnames : {2}\".format(\n",
    "                                                                                row.cedula, best_split, my_pres))\n",
    "    else:\n",
    "        print(\"WARNING at ced {0} - no evids\".format(row.cedula))\n",
    "        flag_split = True\n",
    "    return best_split, flag_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "def extract_prename_parent(row, target_col):\n",
    "    \"\"\" \n",
    "\n",
    "    \"\"\"\n",
    "    target_sur = 'sur_' + target_col.split(\"_\")[1]\n",
    "    sur1 = row[target_sur]\n",
    "    pname = {'cedula':row.cedula,\n",
    "             'sur1':sur1, 'sur2':\"\", 'pre1':\"\", \n",
    "             'pre2':\"\", 'pre3':\"\", 'junk':\"\", 'flag':False}\n",
    "    \n",
    "    n_dup = row[target_col].count(sur1)  # use direct string-count method, to handle \"DE LA CRUZ\", etc\n",
    "    if n_dup > 2:\n",
    "        print(\"ERROR COUNTING DUPS :\", row)\n",
    "        # there's also \"CHEN CHEN SHU CHEN\"\n",
    "        \n",
    "        # workaround for things like \"LEON LEON ALEXANDRA LEONOR\"\n",
    "        is_doubled = row[target_col].split().count(sur1) >= 2\n",
    "    else:\n",
    "        is_doubled = n_dup == 2\n",
    "    if is_doubled:\n",
    "        pname['sur2'] = sur1\n",
    "    \n",
    "    is_pstart = row[target_col].startswith(sur1)\n",
    "    is_pend   = row[target_col].endswith(sur1)\n",
    "\n",
    "    if is_pend :\n",
    "    # name is in normal form; sur2 not present, so everything before sur1 is a prename\n",
    "        pres = ''.join(row[target_col].split(sur1)).strip() # works even if sur1==sur2\n",
    "        pname = parse_pres(pres, pname, row)\n",
    "\n",
    "        if False:\n",
    "            if is_doubled:\n",
    "                pname['sur2'] = sur1\n",
    "            else:\n",
    "                pname['sur2'] = \"\"\n",
    "        \n",
    "    elif not is_pstart:\n",
    "    # name is in normal form, sur2 follows sur1, everything before sur1 is a prename\n",
    "        parts = [x.strip() for x in row[target_col].split(sur1, maxsplit=1)]\n",
    "        \n",
    "        if len(parts) > 1:\n",
    "            pname['sur2'] = parts[1]\n",
    "            pres = row[target_col].split(sur1)[0]\n",
    "            pname = parse_pres(pres, pname, row)\n",
    "        else:\n",
    "            pname['sur2'] = \"WTF MPARSE ERROR\"\n",
    "            pname['flag'] = True\n",
    "        \n",
    "    elif is_pstart:\n",
    "    # name is in legal form, could be short or long version\n",
    "        pres = ''.join(row[target_col].split(sur1)).strip()\n",
    "        \n",
    "        if len(pres.split()) == 1:\n",
    "        # easy case, only thing left must be the prename\n",
    "            pname['pre1'] = pres\n",
    "            \n",
    "        elif is_doubled:\n",
    "        # special case when sur1 == sur2, only thing to do is figure out the prenames\n",
    "            pname = parse_pres(pres, pname, row)\n",
    "        \n",
    "\n",
    "        else:\n",
    "        # harder case.  Could be \"sur1 sur2 pre1 pre2 ...\" or \"sur1 pre1 pre2 ...\"\n",
    "            pre1 = \"\"  # init value; might get clobbered \n",
    "            \n",
    "            # first check if the starting chunk is a multipart name\n",
    "            m_beg = get_starting_multimatch(pres)\n",
    "            if m_beg:\n",
    "            # if it _IS_ a multipart name, it has to be a surname (else it would follow the other prenames)\n",
    "                pname['sur2'] = '_'.join(m_beg.split())\n",
    "                pres = ''.join(pres.split(m_beg))\n",
    "                parts = pres.split()\n",
    "                \n",
    "                if len(parts) > 0:\n",
    "                    pre1 = parts[0]\n",
    "                    if len(parts) > 1:\n",
    "                        pres = ''.join(parts[1:]).strip()\n",
    "                else:\n",
    "                # this triggers when the entire chunk matches a multipart format\n",
    "                    print(\"WTF at {0} - THIS SHOULDNT HAPPEN : {1}\".format(row, row[target_col]))\n",
    "                    pname['flag'] = True\n",
    "                    \n",
    "                    # as a hack, just assume that the final token is a first name.\n",
    "                    pname['pre1'] = m_beg.split()[-1]\n",
    "                    pname['sur2'] = ' '.join(m_beg.split()[:-1])\n",
    "                    return pname\n",
    "\n",
    "            # now check if ending is multipart\n",
    "            m_end = get_ending_multimatch(pres)\n",
    "            if m_end:\n",
    "            # if this person DOES have a multipart prename, it will be at the end.  Extract and continue\n",
    "                pres = ''.join(pres.split(m_end))\n",
    "                m_end = '_'.join(m_end.split())\n",
    "                pres = pres + ' ' + m_end\n",
    "            \n",
    "            # everything left is either a singleton surname, or a prename\n",
    "            if pre1:\n",
    "                my_pres = [pre1] + pres.split()   # the 'pre1' is determined in the \"m_beg\" stage\n",
    "            else:\n",
    "                my_pres = pres.split()\n",
    "                \n",
    "            best_split, flag_split = best_split_by_evidence(my_pres, row)\n",
    "            pname['flag'] = flag_split\n",
    "            \n",
    "            if best_split:\n",
    "                pres = my_pres[best_split:]\n",
    "                pname = assign_pres(pres, pname)\n",
    "                surs = my_pres[:best_split]\n",
    "                if surs:\n",
    "                    pname['sur2'] = surs[0]\n",
    "                \n",
    "    return pname"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Surnames only found for either madre/padre.  Likely some are errors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "surs_madre = set(nf.sur_madre)\n",
    "len(surs_madre)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "surs_padre = set(nf.sur_padre)\n",
    "len(surs_padre)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "surs_both = surs_padre & surs_madre\n",
    "len(surs_both)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "surs_padre - surs_madre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "surs_madre - surs_padre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf[(nf.sur_madre == 'PAYANTI')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf[nf.nombre.map(lambda x: x.startswith(\"LOPEZ LUZURIAGA\"))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf[nf.sur_padre == 'TUNKI']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get padre/madre prenames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "if full_run:\n",
    "    padre_prenames = nf.progress_apply(lambda row: extract_prename_parent(row, 'nombre_padre'), \n",
    "                                       axis=1, result_type='expand')\n",
    "    padre_prenames.to_csv(\"../data/interim/PADRES_\" + TODAY + \".tsv\", sep='\\t', index=False)\n",
    "else:\n",
    "    padre_prenames = pd.read_csv('../data/interim/PADRES_' + READ_DATE + \".tsv\", sep='\\t')\n",
    "\n",
    "# 2h"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del padre_prenames"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "if full_run:\n",
    "    madre_prenames = nf[:1000].progress_apply(lambda row: extract_prename_parent(row, 'nombre_madre'), \n",
    "                                           axis=1, result_type='expand')\n",
    "    madre_prenames.to_csv(\"../data/interim/MADRES_\" + TODAY + \".tsv\", sep='\\t', index=False)\n",
    "else:\n",
    "    madre_prenames = pd.read_csv('../data/interim/MADRES_' + READ_DATE + \".tsv\", sep='\\t')\n",
    "\n",
    "# 2h"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if 'cedula' not in padre_prenames.columns:\n",
    "    padre_prenames['cedula'] = nf.cedula.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(nf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attempt matching (should do this in another notebook)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "nan_values = ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', # 'NA' is sometimes name\n",
    "              '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', '']\n",
    "\n",
    "\n",
    "\n",
    "# read cleaned-up input file\n",
    "print(\"LOADING REG DATA FOR : \" + READ_DATE)\n",
    "dtypes_reg = {'cedula':str, 'nombre':str, 'gender':'category', 'nationality':'category', \n",
    "             'orig_cedula':str, 'marital_status':'category', \n",
    "              'nombre_spouse':str, 'nombre_padre':str, 'nombre_madre':str,\n",
    "              'ced_spouse':str, 'ced_padre':str, 'ced_madre':str\n",
    "             }\n",
    "\n",
    "usecols = ['cedula', 'dt_birth', 'dt_death', 'dt_marriage', 'nombre_spouse',]\n",
    "rf = pd.read_csv(LOC_RAW + \"REG_NAMES_\" + READ_DATE + \".tsv\", sep='\\t', dtype=dtypes_reg,\n",
    "                 parse_dates=['dt_birth','dt_death','dt_marriage'], usecols=usecols,\n",
    "                 keep_default_na=False, na_values=nan_values,\n",
    "                 nrows=N_ROWS\n",
    "                )\n",
    "print(\"Loaded {0} rows\".format(len(rf)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "nf = nf.merge(rf, how='left', on='cedula')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf['nombre_spouse'] = nf.nombre_spouse.fillna('')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del rf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "padre_bothsur = padre_prenames[(padre_prenames.sur2 != \"\") & (nf.dt_birth >= dt.datetime(1960,1,1))]\n",
    "len(padre_bothsur)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "targets = nf[nf.cedula.isin(set(padre_bothsur.cedula))]\n",
    "targets.head(8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target = targets.iloc[1]\n",
    "target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_padre = padre_prenames[padre_prenames.cedula == target.cedula].iloc[0]\n",
    "target_padre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "padre_prenames.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obv_padres = nf[nf.has_padre & nf.is_plegal & (nf.nlen_padre == 4)][['cedula','nombre_padre']]\n",
    "obv_padres"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "whoa = nf.merge(obv_padres.rename(columns={'cedula':'ced_kid', 'nombre_padre':'nombre'}), on='nombre')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(whoa)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "nan_values = ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', # 'NA' is sometimes name\n",
    "              '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', '']\n",
    "\n",
    "# read cleaned-up input file\n",
    "print(\"LOADING REG DATA FOR : \" + READ_DATE)\n",
    "dtypes_reg = {'cedula':str, 'nombre':str, 'gender':'category', 'nationality':'category', \n",
    "             'orig_cedula':str, 'marital_status':'category', \n",
    "              'nombre_spouse':str, 'nombre_padre':str, 'nombre_madre':str,\n",
    "              'ced_spouse':str, 'ced_padre':str, 'ced_madre':str\n",
    "             }\n",
    "\n",
    "usecols = ['cedula', 'ced_padre',]\n",
    "rf = pd.read_csv(LOC_RAW + \"REG_NAMES_\" + READ_DATE + \".tsv\", sep='\\t', dtype=dtypes_reg,\n",
    "                 usecols=usecols,\n",
    "                 keep_default_na=False, na_values=nan_values,\n",
    "                 nrows=N_ROWS\n",
    "                )\n",
    "print(\"Loaded {0} rows\".format(len(rf)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "whoa = whoa.merge(rf, on='cedula', how='left', suffixes=('_pred', '_obs'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "whoa.sample(30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub_pad = nf[(nf.sur_padre == target_padre.sur1) & (nf.sur_madre == target_padre.sur2)\n",
    "             & (nf.gender == 1)\n",
    "            & (nf.dt_birth <= dt.datetime(target.dt_birth.year - 13, target.dt_birth.month, target.dt_birth.day))\n",
    "            ]\n",
    "sub_pad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub_mad = nf[(nf.sur_padre == \"VERGARA\") & (nf.gender == 2)\n",
    "             & (nf.dt_birth <= dt.datetime(target.dt_birth.year - 13, target.dt_birth.month, target.dt_birth.day)) ]\n",
    "len(sub_mad)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "whoa_padre = nf[['cedula','nombre']].rename({'cedula':'ced_padre', 'nombre':'nombre_padre'}, axis=1\n",
    "                    ).merge(nf.loc[(nf.nlen_padre == 4), ['cedula', 'nombre_padre']], on='nombre_padre')\n",
    "print(\"# naive-matched padre :\", len(whoa_padre))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "whoa_madre = nf[['cedula','nombre']].rename({'cedula':'ced_madre', 'nombre':'nombre_madre'}, axis=1\n",
    "                    ).merge(nf.loc[(nf.nlen_padre == 4), ['cedula', 'nombre_madre']], on='nombre_madre')\n",
    "print(\"# naive-matched madre :\", len(whoa_padre))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "if padre starts with surname, it's in legal form.  Check the evidence for other name chunks\n",
    "if padre ends with surname, it's in shortened normal form.  \n",
    "\n",
    "if neither of those, then it's in long normal form.  \n",
    "So everything after the surname should be grandmother, and everything before will be prenames\n",
    "\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_prename(row):\n",
    "    \"\"\" ORiginal version. So. fuckyng. slow (1 hour per million)\"\"\"\n",
    "    sur1 = row.sur_padre\n",
    "    pname = {'sur1':sur1, 'sur2':\"\", 'pre1':\"\", \n",
    "             'pre2':\"\", 'pre3':\"\", 'junk':\"\"}\n",
    "    \n",
    "    is_pstart = row.nombre_padre.startswith(sur1)\n",
    "    is_pend   = row.nombre_padre.endswith(sur1)\n",
    "\n",
    "    if is_pend :\n",
    "    # name is in normal form; sur2 not present, so everything before sur1 is a prename\n",
    "        pres = ''.join(row.nombre_padre.split(sur1))\n",
    "        sur2 = \"\"\n",
    "        pname = parse_pres(pres, pname)\n",
    "        \n",
    "    elif not is_pstart:\n",
    "    # name is in normal form, sur2 follows sur1, everything before sur1 is a prename\n",
    "        parts = row.nombre_padre.split(sur1)\n",
    "        \n",
    "        if len(parts) > 1:\n",
    "            pname['sur2'] = parts[1]\n",
    "            pres = row.nombre_padre.split(sur1)[0]\n",
    "            pname = parse_pres(pres, pname)\n",
    "        else:\n",
    "            pname['sur2'] = \"WTF PPARSE ERROR\"\n",
    "        \n",
    "    elif is_pstart:\n",
    "    # name is in legal form, could be short or long version\n",
    "        pres = ''.join(row.nombre_padre.split(sur1))\n",
    "        \n",
    "        if len(pres.split()) == 1:\n",
    "        # easy case, only thing left must be the prename\n",
    "            pname['pre1'] = pres\n",
    "\n",
    "        else:\n",
    "        # harder case.  Could be \"sur1 sur2 pre1 pre2 ...\" or \"sur1 pre1 pre2 ...\"\n",
    "            pres = fix_funk(pres, funky_prenames).split()\n",
    "            \n",
    "            # check if second token is more likely to be a prename or surname\n",
    "            name_ratio = ncounts[ncounts.obsname == pres[0]]\n",
    "            if (len(name_ratio) > 0) and (name_ratio.iloc[0].sratio > 1):\n",
    "                pname['sur2'] = pres[0]\n",
    "                pres = pres[1:]\n",
    "            \n",
    "            # everything left is a prename\n",
    "            pname['pre1'] = pres[0]\n",
    "            if len(pres) > 1:\n",
    "                pname['pre2'] = pres[1]\n",
    "                if len(pres) > 2:\n",
    "                    pname['pre3'] = pres[2]\n",
    "                    if len(pres) > 3:\n",
    "                        pname['junk'] = ' '.join(pres[4:])\n",
    "\n",
    "    return pname"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_prename(row):\n",
    "    \"\"\" Followup.  I suspect the slowdown happens in the 'elif pstart' clause\n",
    "    \"\"\"\n",
    "    sur1 = row.sur_padre\n",
    "    pname = {'sur1':sur1, 'sur2':\"\", 'pre1':\"\", \n",
    "             'pre2':\"\", 'pre3':\"\", 'junk':\"\"}\n",
    "    \n",
    "    is_pstart = row.nombre_padre.startswith(sur1)\n",
    "    is_pend   = row.nombre_padre.endswith(sur1)\n",
    "\n",
    "    if is_pend :\n",
    "    # name is in normal form; sur2 not present, so everything before sur1 is a prename\n",
    "        pres = ''.join(row.nombre_padre.split(sur1)).strip()\n",
    "        sur2 = \"\"\n",
    "        pname = parse_pres(pres, pname)\n",
    "        \n",
    "    elif not is_pstart:\n",
    "    # name is in normal form, sur2 follows sur1, everything before sur1 is a prename\n",
    "        parts = row.nombre_padre.split(sur1)\n",
    "        \n",
    "        if len(parts) > 1:\n",
    "            pname['sur2'] = parts[1]\n",
    "            pres = row.nombre_padre.split(sur1)[0]\n",
    "            pname = parse_pres(pres, pname)\n",
    "        else:\n",
    "            pname['sur2'] = \"WTF PPARSE ERROR\"\n",
    "        \n",
    "    elif is_pstart:\n",
    "    # name is in legal form, could be short or long version\n",
    "        pres = ''.join(row.nombre_padre.split(sur1)).strip()\n",
    "        \n",
    "        if len(pres.split()) == 1:\n",
    "        # easy case, only thing left must be the prename\n",
    "            pname['pre1'] = pres\n",
    "\n",
    "        else:\n",
    "        # harder case.  Could be \"sur1 sur2 pre1 pre2 ...\" or \"sur1 pre1 pre2 ...\"\n",
    "        \n",
    "            pre1 = \"\"  # init value; might get clobbered \n",
    "            \n",
    "            # first check if the starting chunk is a multipart name\n",
    "            m_beg = get_starting_multimatch(pres)\n",
    "            if m_beg:\n",
    "            # if it _IS_ multipart name, it must be surname (else it would be at the end, following prenames)\n",
    "                pname['sur2'] = m_beg\n",
    "                pres = ''.join(pres.split(m_beg))\n",
    "                parts = pres.split()\n",
    "                if len(parts) > 0:\n",
    "                    pre1 = parts[0]\n",
    "                \n",
    "                    if len(parts) > 1:\n",
    "                        pres = ''.join(parts[1:]).strip()\n",
    "                        \n",
    "                else:\n",
    "                    print(\"WTF THIS SHOULDNT HAPPEN :\", row.nombre_padre)\n",
    "                \n",
    "            \n",
    "            m_end = get_ending_multimatch(pres)\n",
    "            if m_end:\n",
    "            # if this person DOES have a multipart, it will be at the end.  Extract and continue\n",
    "                pres = ''.join(pres.split(m_end))\n",
    "                m_end = '_'.join(m_end.split())\n",
    "            \n",
    "            \n",
    "            # everything left is a prename\n",
    "            pres = [pre1] + pres.split()   # the 'pre1' is possibly determined in the \"m_beg\" stage\n",
    "            pname['pre1'] = pres[0]\n",
    "            if len(pres) > 1:\n",
    "                pname['pre2'] = pres[1]\n",
    "                if len(pres) > 2:\n",
    "                    pname['pre3'] = pres[2]\n",
    "                    if len(pres) > 3:\n",
    "                        pname['junk'] = ' '.join(pres[4:])\n",
    "\n",
    "    return pname"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tst = nf[nf.nombre_padre.map(lambda x: \"DE LA CRUZ \" in x)]\n",
    "\n",
    "def findit(row):\n",
    "    return row.nombre_padre.startswith(row.sur_padre)\n",
    "tst[tst.apply(lambda row: findit(row), axis=1)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_parent_prename(row):\n",
    "    out = {\"pres_padre\":\"\", \"pres_madre\":\"\", \"freq_plegal\":False, \"freq_mlegal\":False}\n",
    "    \n",
    "    is_pstart = row.nombre_padre.startswith(row.sur_padre)\n",
    "    is_pend   = row.nombre_padre.endswith(row.sur_padre)\n",
    "    if is_pstart or is_pend:\n",
    "        out['pres_padre'] = ''.join(row.nombre_padre.split(row.sur_padre)).strip()\n",
    "        out['freq_plegal'] = is_pstart == row.is_plegal\n",
    "    else:\n",
    "        out['pres_padre'] = ' || '.join(row.nombre_padre.split(row.sur_padre))\n",
    "\n",
    "    is_mstart = row.nombre_madre.startswith(row.sur_madre)\n",
    "    is_mend   = row.nombre_madre.endswith(row.sur_madre)\n",
    "    if is_mstart or is_mend:\n",
    "        out['pres_madre'] = ''.join(row.nombre_madre.split(row.sur_madre)).strip()\n",
    "        out['freq_mlegal'] = is_mstart == row.is_mlegal\n",
    "    else:\n",
    "        out['pres_madre'] = ' || '.join(row.nombre_madre.split(row.sur_madre))\n",
    "        \n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_parent_prename(row):\n",
    "    out = {\"pres_padre\":\"\", \"pres_madre\":\"\", \"freq_plegal\":False, \"freq_mlegal\":False}\n",
    "    \n",
    "    is_pstart = row.nombre_padre.startswith(row.sur_padre)\n",
    "    is_pend   = row.nombre_padre.endswith(row.sur_padre)\n",
    "    if is_pstart or is_pend:\n",
    "        out['pres_padre'] = ''.join(row.nombre_padre.split(row.sur_padre)).strip()\n",
    "        out['freq_plegal'] = is_pstart == row.is_plegal\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        out['pres_padre'] = ' || '.join(row.nombre_padre.split(row.sur_padre))\n",
    "\n",
    "    is_mstart = row.nombre_madre.startswith(row.sur_madre)\n",
    "    is_mend   = row.nombre_madre.endswith(row.sur_madre)\n",
    "    if is_mstart or is_mend:\n",
    "        out['pres_madre'] = ''.join(row.nombre_madre.split(row.sur_madre)).strip()\n",
    "        out['freq_mlegal'] = is_mstart == row.is_mlegal\n",
    "    else:\n",
    "        out['pres_madre'] = ' || '.join(row.nombre_madre.split(row.sur_madre))\n",
    "    \n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parse_parents(row, namecounts):\n",
    "    EVIDENCE_THRESH = 10\n",
    "    \n",
    "    out = {'sur2_padre':\"\", 'pre1_padre':\"\", 'pre2_padre':\"\", 'pre3_padre':\"\", 'junk_padre':\"\",\n",
    "           'sur2_madre':\"\", 'pre1_madre':\"\", 'pre2_madre':\"\", 'pre3_madre':\"\", 'junk_madre':\"\",\n",
    "            'cedula':row.cedula\n",
    "          }\n",
    "    try:\n",
    "        # father\n",
    "        if row.is_plegal == True:\n",
    "            tokens = row.nombre_padre.split()\n",
    "            name_rec = namecounts[namecounts.obsname == tokens[1]]\n",
    "            if len(name_rec) == 1:\n",
    "                name_rec = name_rec.iloc[0]\n",
    "    \n",
    "            ppres = ' '.join(tokens[1:])\n",
    "        else:\n",
    "            ppres, psur2 = row.nombre_padre.split(row.sur_padre, maxsplit=1)\n",
    "\n",
    "        pseq = ['pre1_padre', 'pre2_padre', 'pre3_padre', 'junk_padre']\n",
    "        for ind, pre in enumerate(ppres.split()):\n",
    "            if ind < 3:\n",
    "                out[pseq[ind]] = pre\n",
    "            else:\n",
    "                out['junk_padre'] = ' '.join(ppres.split()[3:])\n",
    "        out['sur2_padre'] = psur2\n",
    "\n",
    "        # mother\n",
    "        if row.is_mlegal == True:\n",
    "            tokens = row.nombre_madre.split()\n",
    "            msur2 = tokens[1]\n",
    "            mpres = ' '.join(tokens[1:])\n",
    "        else:\n",
    "            mpres, msur2 = row.nombre_madre.split(row.sur_madre, maxsplit=1)\n",
    "        mseq = ['pre1_madre', 'pre2_madre', 'pre3_madre', 'junk_madre']\n",
    "        for ind, pre in enumerate(mpres.split()):\n",
    "            if ind < 3:\n",
    "                out[mseq[ind]] = pre\n",
    "            else:\n",
    "                out['junk_madre'] = ' '.join(mpres.split()[3:])\n",
    "        out['sur2_madre'] = msur2\n",
    "\n",
    "    except:\n",
    "        print(\"\\nWTF :\\n\", row)\n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# funcs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# in all cases, we look for a word boundary as the first group, then our funky name as the second\n",
    "re_von = re.compile(u\"(^|\\s)(V[AO]N \\w{2,})(\\s|$)\")              # these results are subset of \"re_vande\"\n",
    "re_vande = re.compile(u\"(^|\\s)(V[AO]N DE[RN]? \\w{2,})(\\s|$)\")\n",
    "re_sant = re.compile(u\"(^|\\s)(SANT?A? \\w{2,})(\\s|$)\")\n",
    "re_dela = re.compile(u\"(^|\\s)(DE L[AO]S? ?[AO]? ?\\w{2,})(\\s|$)\")   # these results are subset of \"re_laos\"\n",
    "re_laos = re.compile(u\"(^|\\s)(L[AEO]S? \\w{2,})(\\s|$)\")\n",
    "re_del  = re.compile(u\"(^|\\s)(DEL \\w{2,})(\\s|$)\")\n",
    "re_de   = re.compile(r\"(^|\\s)(DE \\w{2,})(\\s|$)\")\n",
    "\n",
    "\n",
    "def regex_funky(nombre):\n",
    "    \"\"\" This is a little slow (~4mins / million rows), but pretty thorough.  \"\"\"\n",
    "    \n",
    "    mdel   = re_del.search(nombre)\n",
    "    msant  = re_sant.search(nombre)\n",
    "    \n",
    "    mlaos  = re_laos.search(nombre)\n",
    "    mdela  = re_dela.search(nombre)\n",
    "    \n",
    "    mvon   = re_von.search(nombre)\n",
    "    mvande = re_vande.search(nombre)\n",
    "    \n",
    "    poss_funks = set()\n",
    "    \n",
    "    if mdel:\n",
    "        poss_funks.add(mdel.group(2))\n",
    "    if msant:\n",
    "        poss_funks.add(msant.group(2))\n",
    "    if mvon:\n",
    "        # \"VAN DE\" types are a subset of \"VAN\" types\n",
    "        if mvande:\n",
    "            poss_funks.add(mvande.group(2))\n",
    "        else:\n",
    "            poss_funks.add(mvon.group(2))\n",
    "    if mlaos:\n",
    "        # \"DE LA\" type names are a subset of \"LA\" types\n",
    "        if mdela:\n",
    "            poss_funks.add(mdela.group(2))\n",
    "        else:\n",
    "            poss_funks.add(mlaos.group(2))\n",
    "\n",
    "    if poss_funks:\n",
    "        for funk in poss_funks:\n",
    "            funky_prenames.add(funk)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf = rf.loc[(rf.sur_padre != \"\") & rf.has_padre & (rf.sur_madre != \"\") & rf.has_madre & (rf.prenames != \"\"),\n",
    "        ['cedula','nombre','prenames', 'gender', 'nombre_padre','sur_padre','has_padre','nombre_madre','sur_madre','has_madre']]\n",
    "len(nf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "funky_prenames = set()\n",
    "nf['is_funky'] = nf.prenames.map(regex_funky)\n",
    "print(\"# funkies :\", len(funky_prenames))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "funky_prenames = list(funky_prenames)\n",
    "funky_prenames.sort(reverse=True, key=len)\n",
    "len(funky_prenames)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "def fix_funk(nombre, funks):\n",
    "    \"\"\" The 'funks' list should be sorted in descending length, to prevent substrings from being clobbered.\n",
    "    \n",
    "    NB: there's a potential bug in here, bc the list is sorted according to character length, but checks\n",
    "    here are being done according to number of tokens.  But very unlikely to cause an issue, so ignoring for now\n",
    "    \"\"\"\n",
    "    nlen = len(nombre.split())\n",
    "    for funk in funks:\n",
    "        flen = len(funk.split())\n",
    "        if (nlen >= flen):\n",
    "            if (funk in nombre):\n",
    "                defunk = '_'.join(funk.split())\n",
    "                nombre = defunk.join(nombre.split(funk))\n",
    "                nlen = len(nombre.split())\n",
    "        else:\n",
    "            # since the list is sorted, once we have a match that uses all the tokens, just skip ahead\n",
    "            continue\n",
    "    return nombre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nf.loc[nf.is_funky, 'prenames'] = nf[nf.is_funky].prenames.map(lambda x: fix_funk(x, funky_prenames))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parse_prename(prenames):\n",
    "    \"\"\" The surnames are parsed, but the prenames must \"\"\"\n",
    "    \n",
    "    out = {'pre1':\"\", 'pre2':\"\", 'pre3':\"\", 'junk':\"\"}\n",
    "    \n",
    "    mdel   = re_del.search(prenames)\n",
    "    msant  = re_sant.search(prenames)\n",
    "    \n",
    "    mlaos  = re_laos.search(prenames)\n",
    "    mdela  = re_dela.search(prenames)\n",
    "    \n",
    "    mde  = re_de.search(prenames)\n",
    "    \n",
    "    mvon   = re_von.search(prenames)\n",
    "    mvande = re_vande.search(prenames)\n",
    "    \n",
    "    curr_funks = set()  \n",
    "    if mdel:\n",
    "        curr_funks.add(mdel.group(2))\n",
    "        \n",
    "    if mde:\n",
    "        curr_funks.add(mde.group(2))\n",
    "    if msant:\n",
    "        curr_funks.add(msant.group(2))\n",
    "    if mvon:\n",
    "        # \"VAN DE\" types are a subset of \"VAN\" types\n",
    "        if mvande:\n",
    "            curr_funks.add(mvande.group(2))\n",
    "        else:\n",
    "            curr_funks.add(mvon.group(2))\n",
    "    if mlaos:\n",
    "        # \"DE LA\" type names are a subset of \"LA\" types\n",
    "        if mdela:\n",
    "            curr_funks.add(mdela.group(2))\n",
    "        else:\n",
    "            curr_funks.add(mlaos.group(2))\n",
    "            \n",
    "    # sort greedily, first by number of tokens, and then by number of characters\n",
    "    curr_funks = list(curr_funks)\n",
    "    curr_funks.sort(reverse=True, key=lambda x: (len(x.split()), len(x)))\n",
    "    \n",
    "    for funk in curr_funks:\n",
    "        if len(funk) < len(prenames.split()):\n",
    "            continue\n",
    "            \n",
    "        parts = prenames.split(funk)\n",
    "        sub = \"_\".join(funk.split())\n",
    "        if len(parts) == 2:\n",
    "            if parts[0] == \"\":\n",
    "                # match was at beginning of the string\n",
    "                prenames = \" \".join([sub] + parts)\n",
    "            elif parts[1] == \"\":\n",
    "                # match was at end of the string\n",
    "                prenames = \" \".join(parts + [sub])\n",
    "            else:\n",
    "                prenames = parts[0] + sub + parts[1]\n",
    "        \n",
    "        # these shouldn't happen\n",
    "        elif len(parts) > 2:\n",
    "            print(\"TOOLONG :\", prenames, parts)\n",
    "    \n",
    "    # now assign name pices\n",
    "    pres = prenames.split()\n",
    "    if len(pres) >= 1:\n",
    "        out['pre1'] = pres[0]\n",
    "    if len(pres) >= 2:\n",
    "        out['pre2'] = pres[1]\n",
    "    if len(pres) >= 3:\n",
    "        out['pre3'] = pres[2]\n",
    "    if len(pres) >= 4:\n",
    "        out['junk'] = ' '.join(pres[3:])\n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parse_prename(prenames):\n",
    "    \"\"\" The surnames are parsed, but the prenames must \"\"\"\n",
    "    \n",
    "    out = {'pre1':\"\", 'pre2':\"\", 'pre3':\"\", 'junk':\"\"}\n",
    "    \n",
    "    # now assign name pices\n",
    "    pres = prenames.split()\n",
    "    if len(pres) >= 1:\n",
    "        out['pre1'] = pres[0]\n",
    "    if len(pres) >= 2:\n",
    "        out['pre2'] = pres[1]\n",
    "    if len(pres) >= 3:\n",
    "        out['pre3'] = pres[2]\n",
    "    if len(pres) >= 4:\n",
    "        out['junk'] = ' '.join(pres[3:])\n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tmp = pd.concat([freq.sur_padre, freq.sur_madre], axis=0).value_counts()\n",
    "count_sur = pd.DataFrame({'obsname':tmp.index, 'n_sur':tmp.values})\n",
    "count_sur.sample(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tmp = pd.concat([freq.pre1, freq.pre2], axis=0).value_counts()\n",
    "count_pre = pd.DataFrame({'obsname':tmp.index, 'n_pre':tmp.values})\n",
    "count_pre.sample(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_names = count_sur.merge(count_pre, on='obsname', how='outer')\n",
    "count_names.fillna(0, inplace=True)\n",
    "\n",
    "# add null record, so that null names get weight factor of 1\n",
    "#count_names = count_names.append({'obsname':np.nan, 'n_sur':0, 'n_pre':0}, ignore_index=True)\n",
    "\n",
    "count_names.loc[count_names.obsname == \"\", ['n_sur','n_pre']] = 0\n",
    "count_names.sample(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_names['n_sur'] = count_names.n_sur + 0.5\n",
    "count_names['n_pre'] = count_names.n_pre + 0.5\n",
    "\n",
    "count_names['sratio'] = count_names.n_sur / count_names.n_pre\n",
    "count_names['pratio'] = count_names.n_pre / count_names.n_sur"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_names[count_names.obsname == \"\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Investigate multinames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def is_name_multimatch(nombre):\n",
    "    mdel   = re_del.search(nombre)\n",
    "    msant  = re_sant.search(nombre)\n",
    "    \n",
    "    mlaos  = re_laos.search(nombre)\n",
    "    mdela  = re_dela.search(nombre)\n",
    "    \n",
    "    mde  = re_de.search(nombre)\n",
    "    \n",
    "    mvon   = re_von.search(nombre)\n",
    "    mvande = re_vande.search(nombre)\n",
    "    \n",
    "    if mdel or msant or mlaos or mdela or mde or mvon or mvande:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \"\"\"\n",
    "    if mdel:\n",
    "        curr_funks.add(mdel.group(2))\n",
    "        \n",
    "    if mde:\n",
    "        curr_funks.add(mde.group(2))\n",
    "    if msant:\n",
    "        curr_funks.add(msant.group(2))\n",
    "    if mvon:\n",
    "        # \"VAN DE\" types are a subset of \"VAN\" types\n",
    "        if mvande:\n",
    "            curr_funks.add(mvande.group(2))\n",
    "        else:\n",
    "            curr_funks.add(mvon.group(2))\n",
    "    if mlaos:\n",
    "        # \"DE LA\" type names are a subset of \"LA\" types\n",
    "        if mdela:\n",
    "            curr_funks.add(mdela.group(2))\n",
    "        else:\n",
    "            curr_funks.add(mlaos.group(2))\n",
    "    \"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_names['nlen'] = count_names.obsname.map(lambda x: len(x.split()))\n",
    "count_names['is_multimatch'] = count_names.obsname.map(is_name_multimatch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_names[(count_names.nlen == 3) & count_names.is_multimatch]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub = count_names[(count_names.nlen == 2) & ~count_names.is_multimatch]\n",
    "len(sub)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_name_splitting(obsname):\n",
    "    tokens = obsname.split()\n",
    "    \n",
    "    try:\n",
    "        probably_sur = count_names[count_names.obsname == tokens[0]].iloc[0].n_sur\n",
    "    except IndexError:\n",
    "        probably_sur = 0.5\n",
    "        \n",
    "    if tokens[1] == 'MARIA':\n",
    "        probably_pre = 1107271.5\n",
    "    else:\n",
    "        try:\n",
    "            probably_pre = count_names[count_names.obsname == tokens[1]].iloc[0].n_sur\n",
    "        except IndexError:\n",
    "            probably_pre = 0.5\n",
    "        \n",
    "    return probably_sur / probably_pre"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calc_name_ratios(df):\n",
    "    # count of surnames & prenames\n",
    "    tmp = pd.concat([df.sur1,df.sur2], axis=0).value_counts()\n",
    "    count_sur = pd.DataFrame({'obsname':tmp.index, 'n_sur':tmp.values})\n",
    "    tmp = pd.concat([df.pre1,df.pre2], axis=0).value_counts()\n",
    "    count_pre = pd.DataFrame({'obsname':tmp.index, 'n_pre':tmp.values})\n",
    "\n",
    "    # merge to single list of names\n",
    "    count_names = count_sur.merge(count_pre, on='obsname', how='outer')\n",
    "    count_names.fillna(0, inplace=True)\n",
    "\n",
    "    # add null record, so that null names get weight factor of 1\n",
    "    count_names = count_names.append({'obsname':np.nan, 'n_sur':0, 'n_pre':0}, ignore_index=True)\n",
    "    \n",
    "    # laplace prior\n",
    "    count_names['n_sur'] = count_names.n_sur + 0.5\n",
    "    count_names['n_pre'] = count_names.n_pre + 0.5\n",
    "\n",
    "    # evidence for each instance being surname or prename\n",
    "    count_names['sratio'] = count_names.n_sur / count_names.n_pre\n",
    "    count_names['pratio'] = count_names.n_pre / count_names.n_sur\n",
    "    \n",
    "    return count_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calc_name_evidence(df, count_names):\n",
    "    df['wt_sur1'] = df.join(count_names[['obsname','sratio']].set_index('obsname'), on='sur1')['sratio']\n",
    "    df['wt_sur2'] = df.join(count_names[['obsname','sratio']].set_index('obsname'), on='sur2')['sratio']\n",
    "    df['wt_pre1'] = df.join(count_names[['obsname','pratio']].set_index('obsname'), on='pre1')['pratio']\n",
    "    df['wt_pre2'] = df.join(count_names[['obsname','pratio']].set_index('obsname'), on='pre2')['pratio']\n",
    "    df['wt_pre3'] = df.join(count_names[['obsname','pratio']].set_index('obsname'), on='pre3')['pratio']\n",
    "\n",
    "    df['evidence'] = df.wt_sur1 * df.wt_sur2 * df.wt_pre1 * df.wt_pre2 * df.wt_pre3\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def alt_schemes_2len(row, threshold=100):\n",
    "    new = row.copy()\n",
    "    \n",
    "    alt_evidence = 1/row.evidence\n",
    "    if alt_evidence > threshold:\n",
    "        new.sur1 = row.pre1\n",
    "        new.pre1 = row.sur1\n",
    "        new.wt_sur1 = 1/row.wt_pre1\n",
    "        new.wt_pre1 = 1/row.wt_sur1\n",
    "        new.evidence = alt_evidence\n",
    "    return new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def alt_schemes_3len(row, threshold=100):\n",
    "    \n",
    "    new = row.copy()\n",
    "        \n",
    "    # first check to be sure this isn't a double-name\n",
    "    if row.sur1 == row.sur2:\n",
    "        return new\n",
    "    \n",
    "    comp_evids = [row.evidence]\n",
    "    \n",
    "    # s1 p1 p2 (person has 2 prenames; only middle column has wrong weighting)\n",
    "    comp_evids.append( row.wt_sur1 * (1/row.wt_sur2) * row.wt_pre1 )\n",
    "    \n",
    "    # p1 s1 s2 (sequence is wrong; middle col has correct weight)\n",
    "    comp_evids.append( (1/row.wt_sur1) * row.wt_sur2 * (1/row.wt_pre1) )\n",
    "    \n",
    "    # p1 p2 s1 (everything is wrong )\n",
    "    comp_evids.append( (1/row.wt_sur1) * (1/row.wt_sur2) * (1/row.wt_pre1) )\n",
    "    \n",
    "    # get index of permutation with highest evidence\n",
    "    best_perm = np.array(comp_evids).argmax()\n",
    "#    print(comp_evids)\n",
    "    \n",
    "    # check if best alternative is higher than our prior before continuing\n",
    "    if comp_evids[best_perm] < threshold:\n",
    "        pass\n",
    "    \n",
    "    elif best_perm == 0:\n",
    "        # original version really was the best\n",
    "        pass\n",
    "        \n",
    "    elif best_perm == 1:\n",
    "        # s1 s2 p1\n",
    "        # s1 p1 p2 (person has 2 prenames; only middle column has wrong weighting)\n",
    "        new = row.copy()\n",
    "        \n",
    "        new.sur2 = np.nan\n",
    "        new.pre1 = row.sur2\n",
    "        new.pre2 = row.pre1\n",
    "        \n",
    "        new.evidence = comp_evids[best_perm]\n",
    "        new.wt_sur2 = 1\n",
    "        new.wt_pre1 = 1/row.wt_sur2\n",
    "        new.wt_pre2 = 1/row.wt_pre1\n",
    "        \n",
    "    elif best_perm == 2:\n",
    "        # s1 s2 p1\n",
    "        # p1 s1 s2 (sequence is wrong; middle col has correct weight)\n",
    "        new = row.copy()\n",
    "        \n",
    "        new.sur1 = row.sur2\n",
    "        new.sur2 = row.pre1\n",
    "        new.pre1 = row.sur1\n",
    "        \n",
    "        new.evidence = comp_evids[best_perm]\n",
    "        new.wt_sur1 = row.wt_sur2\n",
    "        new.wt_sur2 = 1/row.wt_pre1\n",
    "        new.wt_pre1 = 1/row.wt_sur1\n",
    "        \n",
    "    elif best_perm == 3:\n",
    "        # p1 p2 s1 (everything is wrong )\n",
    "        \n",
    "        new.sur1 = row.pre1\n",
    "        new.sur2 = np.nan\n",
    "        new.pre1 = row.sur1\n",
    "        new.pre2 = row.sur2\n",
    "        \n",
    "        new.evidence = comp_evids[best_perm]\n",
    "        new.wt_sur1 = 1/row.wt_pre1\n",
    "        new.wt_sur2 = 1\n",
    "        new.wt_pre1 = 1/row.wt_sur1\n",
    "        new.wt_pre2 = 1/row.wt_sur2\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"How did we even get here?\")\n",
    "        \n",
    "    return new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def alt_schemes_4len(row, threshold=100):\n",
    "    \n",
    "    new = row.copy()\n",
    "    \n",
    "    # first check to be sure this isn't a double-name\n",
    "    if row.sur1 == row.sur2:\n",
    "        return new\n",
    "    \n",
    "    ### calculate evidence in each possible configuration ###\n",
    "    comp_evids = [row.evidence]\n",
    "    \n",
    "    # p1 p2 s1 s2 (every col has wrong weighting, so take reciprocal of all of them)\n",
    "    comp_evids.append( (1/row.wt_sur1) * (1/row.wt_sur2) * (1/row.wt_pre1) * (1/row.wt_pre2) * row.wt_pre3 )\n",
    "    \n",
    "    # s1 p1 p2 p3 (only second col has wrong weighting)\n",
    "    comp_evids.append( row.wt_sur1 * (1/row.wt_sur2) * row.wt_pre1 * row.wt_pre2 * row.wt_pre3)\n",
    "    \n",
    "    # p1 p2 p3 s1 (only third col _doesn't have the wrong weighting)\n",
    "    comp_evids.append( (1/row.wt_sur1) * (1/row.wt_sur2) * row.wt_pre1 * (1/row.wt_pre2) * row.wt_pre3)\n",
    "    \n",
    "    \n",
    "    # get index of permutation with highest evidence\n",
    "    best_perm = np.array(comp_evids).argmax()\n",
    "    \n",
    "    if row.pre1 == row.pre2:\n",
    "        # person has two last names, and reversed sequence. Force this even if it's a weak last name (eg DAMIAN)\n",
    "        best_perm = 1\n",
    "    \n",
    "    # check if best alternative is higher than our prior before continuing\n",
    "    if comp_evids[best_perm] < threshold:\n",
    "        pass\n",
    "    elif best_perm == 0:\n",
    "        # original version really was the best\n",
    "        pass\n",
    "        \n",
    "    elif best_perm == 1:\n",
    "        # p1 p2 s1 s2 (flipped from normal)\n",
    "        new.pre1 = row.sur1\n",
    "        new.pre2 = row.sur2\n",
    "        new.sur1 = row.pre1\n",
    "        new.sur2 = row.pre2\n",
    "        new.pre3 = np.nan\n",
    "        new.evidence = comp_evids[best_perm]\n",
    "        new.wt_pre1 = 1/row.wt_sur1\n",
    "        new.wt_pre2 = 1/row.wt_sur2\n",
    "        new.wt_sur1 = 1/row.wt_pre1\n",
    "        new.wt_sur2 = 1/row.wt_pre2\n",
    "        new.wt_pre3 = 1\n",
    "        \n",
    "    elif best_perm == 2:\n",
    "        # s1 p1 p2 p3 (person has correct order, but 3 prenames)\n",
    "#        new.sur1 = row.sur1  # no change\n",
    "        new.pre1 = row.sur2\n",
    "        new.pre2 = row.pre1\n",
    "        new.pre3 = row.pre2\n",
    "        new.sur2 = np.nan\n",
    "        new.evidence = comp_evids[best_perm]\n",
    "#        new.wt_sur1 = row.wt_sur1\n",
    "        new.wt_pre1 = 1/row.wt_sur2\n",
    "        new.wt_pre2 = 1/row.wt_pre1\n",
    "        new.wt_pre3 = 1/row.wt_pre2\n",
    "        new.wt_sur2 = 1\n",
    "        \n",
    "    elif best_perm == 3:\n",
    "        # p1 p2 p3 s1 (person has wrong order and 3 prenames)\n",
    "        new.pre1 = row.sur1\n",
    "        new.pre2 = row.sur2\n",
    "        new.pre3 = row.pre1\n",
    "        new.sur1 = row.pre2\n",
    "        new.sur2 = np.nan\n",
    "        \n",
    "        new.evidence = comp_evids[best_perm]\n",
    "        new.wt_pre1 = row.wt_sur1\n",
    "        new.wt_pre2 = 1/row.wt_sur2\n",
    "        new.wt_pre3 = 1/row.wt_pre1\n",
    "        new.wt_sur1 = 1/row.wt_pre2\n",
    "        new.wt_sur2 = 1\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"How did we even get here?\")\n",
    "    return new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_name_permutations(row, threshold=None):\n",
    "    if threshold is None:\n",
    "        threshold = np.min([1000*row.evidence, 100])\n",
    "    \n",
    "    if row.nlen == 2:\n",
    "        return alt_schemes_2len(row, threshold)\n",
    "    elif row.nlen == 3:\n",
    "        return alt_schemes_3len(row, threshold)\n",
    "    elif row.nlen == 4:\n",
    "        return alt_schemes_4len(row, threshold)\n",
    "    else:\n",
    "        # don't bother for the hard ones\n",
    "        return row"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def name_ordering(row):\n",
    "    row.nombre_padre.split()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "freq['sur1'] = freq.sur1.astype('category')\n",
    "freq['sur2'] = freq.sur2.astype('category')\n",
    "freq['pre1'] = freq.pre1.astype('category')\n",
    "freq['pre2'] = freq.pre2.astype('category')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def set_nanfree(vals):\n",
    "    \"\"\" Makes a nan-free set from an iterable.\n",
    "    \n",
    "    NaN can behave wierd in sets/dicts, because there are actually 16M possible NaN.\n",
    "    see https://github.com/numpy/numpy/issues/9358\n",
    "    \"\"\"\n",
    "    myset = set()\n",
    "    for val in vals:\n",
    "        try:\n",
    "            if not np.isnan(val):\n",
    "                myset.add(val)\n",
    "        except TypeError:\n",
    "            myset.add(val)\n",
    "    return myset\n",
    "\n",
    "\n",
    "def prepare_catcol_merge(df, nf):\n",
    "    sur1 = set_nanfree(nf.sur1.values)\n",
    "    sur2 = set_nanfree(nf.sur2.values)\n",
    "    pre1 = set_nanfree(nf.pre1.values)\n",
    "    pre2 = set_nanfree(nf.pre2.values)\n",
    "    \n",
    "    df['sur1'] = df.sur1.cat.add_categories(sur1 - set(df.sur1.cat.categories))\n",
    "    df['sur2'] = df.sur2.cat.add_categories(sur2 - set(df.sur2.cat.categories))\n",
    "    df['pre1'] = df.pre1.cat.add_categories(pre1 - set(df.pre1.cat.categories))\n",
    "    df['pre2'] = df.pre2.cat.add_categories(pre2 - set(df.pre2.cat.categories))\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepare_catcol_merge(df, nf):\n",
    "    new_sur1 = set(nf.sur1) - set(df.sur1.cat.categories)\n",
    "    new_sur2 = set(nf.sur2) - set(df.sur2.cat.categories)\n",
    "    new_pre1 = set(nf.pre1) - set(df.pre1.cat.categories)\n",
    "    new_pre2 = set(nf.pre2) - set(df.pre2.cat.categories)\n",
    "    \n",
    "    df['sur1'] = df.sur1.cat.add_categories(new_sur1)\n",
    "    df['sur2'] = df.sur2.cat.add_categories(new_sur2)\n",
    "    df['pre1'] = df.pre1.cat.add_categories(new_pre1)\n",
    "    df['pre2'] = df.pre2.cat.add_categories(new_pre2)\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check output of swapping\n",
    "\n",
    "So the \"parse_fullrow\" method works so well, that this should only be used in cases where there weren't parent names to match.\n",
    "\n",
    "HOWEVER, it's still great for parsing out first/last names of the parents\n",
    "\n",
    "PARENT_PARSE:\n",
    "\n",
    "should be done by comparing the extracted surname against the position within the parent appelidos.  Probably ought to do male names first, and identify funky versions.  The extracted surname ought to correspond to position 0 (if in normal form) or to position N-1 (if in alternate form).  In alternate form, the remainder of the name can be safely assigned as a surname.  In standard form, follow the \"madre approach\" to identify the remainder.  Establish ratio of normal/alternate form.  Establish \"mismatch ratio\" for when parent forms aren't concordant.\n",
    "\n",
    "For the ~9% of records without both matching parents, do a few other checks. \n",
    "1) if the entirety of the parent name is found within the child (i.e. no parent prename), flag as a \"junior-style\" name\n",
    "2) for other names, compute a levenshtein similarity, update parent to match child\n",
    "3) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iters = []\n",
    "avgs = []\n",
    "stds = []\n",
    "\n",
    "wtfs = []\n",
    "subs = []\n",
    "\n",
    "run = True\n",
    "ind = 0\n",
    "old_avg = 1\n",
    "while run:\n",
    "    print('iteration', ind)\n",
    "    count_names = calc_name_ratios(freq)\n",
    "    freq = calc_name_evidence(freq, count_names)\n",
    "\n",
    "    print(\"{0}, {1:.3e}, {2:.3e}\".format(ind, freq.evidence.mean(), freq.evidence.std()))\n",
    "    iters.append(ind)\n",
    "    avgs.append(freq.evidence.mean())\n",
    "    stds.append(freq.evidence.std())\n",
    "\n",
    "    sub = freq[(freq.evidence < 2**ind) ].copy(deep=True)\n",
    "    print(\"# susp :\", len(sub))\n",
    "    tmp = sub.apply(lambda row: check_name_permutations(row), axis=1, result_type='expand')\n",
    "    subs.append(sub.copy(deep=True))\n",
    "    wtfs.append(tmp.copy(deep=True))\n",
    "    tmp = tmp[tmp.cedula.notnull()]\n",
    "    tmp.replace(np.nan, \"\", inplace=True)\n",
    "\n",
    "    freq = prepare_catcol_merge(freq, tmp)\n",
    "    freq.loc[freq.cedula.isin(sub.cedula),:] = tmp\n",
    "    \n",
    "    if abs(avgs[-1] - old_avg)/old_avg < 0.01:\n",
    "        run = False\n",
    "    if ind >= 10:\n",
    "        run = False\n",
    "    old_avg = avgs[-1]\n",
    "    ind += 1"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}